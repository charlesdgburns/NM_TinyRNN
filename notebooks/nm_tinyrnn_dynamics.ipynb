{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charlesdgburns/NM_TinyRNN/blob/main/notebooks/nm_tinyrnn_dynamics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5W-ZRWU03M9"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Let's use this as a learning notebook about different RNN architectures.\n",
        "We want to start from scratch and get an idea of how different architectures work before fitting to some data.\n",
        "\n",
        "\n",
        "We the fit these to sequential behavioural decision making later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaPBpmAS_mMk",
        "outputId": "083a4bcf-4ae7-44f0-999e-8537c7c86660"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'NM_TinyRNN'...\n",
            "remote: Enumerating objects: 104, done.\u001b[K\n",
            "remote: Counting objects: 100% (104/104), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 104 (delta 37), reused 55 (delta 17), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (104/104), 212.00 KiB | 4.93 MiB/s, done.\n",
            "Resolving deltas: 100% (37/37), done.\n"
          ]
        }
      ],
      "source": [
        "## setup on google colab:\n",
        "\n",
        "!git clone https://github.com/charlesdgburns/NM_TinyRNN.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XNzAHyfqAuHq"
      },
      "outputs": [],
      "source": [
        "# setup\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import os\n",
        "CODE_DIR = Path(Path(r'C:\\Users\\owner\\Research'))\n",
        "os.chdir(CODE_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "psieepNUAqte"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training with 4 sparsity values...\n",
            "Dataset size: 28\n",
            "Split sizes - Train: 22, Val: 2, Test: 4\n",
            "\n",
            "Training with sparsity lambda = 0.01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "λ=1e-02: 100%|██████████| 10/10 [00:02<00:00,  4.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training with sparsity lambda = 0.001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "λ=1e-03: 100%|██████████| 10/10 [00:02<00:00,  4.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training with sparsity lambda = 0.0001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "λ=1e-04: 100%|██████████| 10/10 [00:02<00:00,  4.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training with sparsity lambda = 1e-05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "λ=1e-05: 100%|██████████| 10/10 [00:02<00:00,  4.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating best model (λ=1e-05) on test set...\n",
            "Evaluation loss: 0.673868\n",
            "\n",
            "Training complete!\n",
            "Best model (λ=1e-05) saved to: C:\\Users\\owner\\Research\\NM_TinyRNN\\data\\rnns\\GRU\n",
            "Best validation loss: 0.676340\n",
            "Test loss: 0.673868\n",
            "Lastly, extracting activations on full dataset\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sparsity_lambda</th>\n",
              "      <th>epoch</th>\n",
              "      <th>val_pred_loss</th>\n",
              "      <th>train_pred_loss</th>\n",
              "      <th>train_sparsity_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.01000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.688561</td>\n",
              "      <td>0.688144</td>\n",
              "      <td>0.000570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.01000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.685777</td>\n",
              "      <td>0.685254</td>\n",
              "      <td>0.000577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.01000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.683678</td>\n",
              "      <td>0.681928</td>\n",
              "      <td>0.000584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.01000</td>\n",
              "      <td>3</td>\n",
              "      <td>0.682117</td>\n",
              "      <td>0.684018</td>\n",
              "      <td>0.000589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.01000</td>\n",
              "      <td>4</td>\n",
              "      <td>0.680886</td>\n",
              "      <td>0.683521</td>\n",
              "      <td>0.000593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.01000</td>\n",
              "      <td>5</td>\n",
              "      <td>0.679893</td>\n",
              "      <td>0.680334</td>\n",
              "      <td>0.000595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.01000</td>\n",
              "      <td>6</td>\n",
              "      <td>0.679038</td>\n",
              "      <td>0.680134</td>\n",
              "      <td>0.000596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.01000</td>\n",
              "      <td>7</td>\n",
              "      <td>0.678264</td>\n",
              "      <td>0.679861</td>\n",
              "      <td>0.000596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.01000</td>\n",
              "      <td>8</td>\n",
              "      <td>0.677501</td>\n",
              "      <td>0.677641</td>\n",
              "      <td>0.000595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.01000</td>\n",
              "      <td>9</td>\n",
              "      <td>0.676726</td>\n",
              "      <td>0.678914</td>\n",
              "      <td>0.000594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.00100</td>\n",
              "      <td>0</td>\n",
              "      <td>0.688556</td>\n",
              "      <td>0.688894</td>\n",
              "      <td>0.000569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.00100</td>\n",
              "      <td>1</td>\n",
              "      <td>0.685788</td>\n",
              "      <td>0.685571</td>\n",
              "      <td>0.000577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.00100</td>\n",
              "      <td>2</td>\n",
              "      <td>0.683697</td>\n",
              "      <td>0.683745</td>\n",
              "      <td>0.000583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.00100</td>\n",
              "      <td>3</td>\n",
              "      <td>0.682122</td>\n",
              "      <td>0.683165</td>\n",
              "      <td>0.000588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.00100</td>\n",
              "      <td>4</td>\n",
              "      <td>0.680895</td>\n",
              "      <td>0.682601</td>\n",
              "      <td>0.000592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.00100</td>\n",
              "      <td>5</td>\n",
              "      <td>0.679934</td>\n",
              "      <td>0.682531</td>\n",
              "      <td>0.000594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.00100</td>\n",
              "      <td>6</td>\n",
              "      <td>0.679097</td>\n",
              "      <td>0.680332</td>\n",
              "      <td>0.000595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.00100</td>\n",
              "      <td>7</td>\n",
              "      <td>0.678308</td>\n",
              "      <td>0.679547</td>\n",
              "      <td>0.000594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.00100</td>\n",
              "      <td>8</td>\n",
              "      <td>0.677521</td>\n",
              "      <td>0.679869</td>\n",
              "      <td>0.000593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.00100</td>\n",
              "      <td>9</td>\n",
              "      <td>0.676719</td>\n",
              "      <td>0.677866</td>\n",
              "      <td>0.000591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.00010</td>\n",
              "      <td>0</td>\n",
              "      <td>0.688564</td>\n",
              "      <td>0.686957</td>\n",
              "      <td>0.000570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.00010</td>\n",
              "      <td>1</td>\n",
              "      <td>0.685796</td>\n",
              "      <td>0.686587</td>\n",
              "      <td>0.000578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.00010</td>\n",
              "      <td>2</td>\n",
              "      <td>0.683633</td>\n",
              "      <td>0.684558</td>\n",
              "      <td>0.000585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.00010</td>\n",
              "      <td>3</td>\n",
              "      <td>0.681989</td>\n",
              "      <td>0.682575</td>\n",
              "      <td>0.000590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.00010</td>\n",
              "      <td>4</td>\n",
              "      <td>0.680755</td>\n",
              "      <td>0.680686</td>\n",
              "      <td>0.000595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.00010</td>\n",
              "      <td>5</td>\n",
              "      <td>0.679801</td>\n",
              "      <td>0.680708</td>\n",
              "      <td>0.000597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.00010</td>\n",
              "      <td>6</td>\n",
              "      <td>0.678986</td>\n",
              "      <td>0.681541</td>\n",
              "      <td>0.000598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.00010</td>\n",
              "      <td>7</td>\n",
              "      <td>0.678227</td>\n",
              "      <td>0.680874</td>\n",
              "      <td>0.000598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.00010</td>\n",
              "      <td>8</td>\n",
              "      <td>0.677469</td>\n",
              "      <td>0.679508</td>\n",
              "      <td>0.000597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.00010</td>\n",
              "      <td>9</td>\n",
              "      <td>0.676701</td>\n",
              "      <td>0.679961</td>\n",
              "      <td>0.000595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.00001</td>\n",
              "      <td>0</td>\n",
              "      <td>0.688676</td>\n",
              "      <td>0.686500</td>\n",
              "      <td>0.000570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.00001</td>\n",
              "      <td>1</td>\n",
              "      <td>0.685974</td>\n",
              "      <td>0.684515</td>\n",
              "      <td>0.000577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.00001</td>\n",
              "      <td>2</td>\n",
              "      <td>0.683793</td>\n",
              "      <td>0.683916</td>\n",
              "      <td>0.000584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.00001</td>\n",
              "      <td>3</td>\n",
              "      <td>0.682112</td>\n",
              "      <td>0.681476</td>\n",
              "      <td>0.000590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.00001</td>\n",
              "      <td>4</td>\n",
              "      <td>0.680840</td>\n",
              "      <td>0.680068</td>\n",
              "      <td>0.000594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.00001</td>\n",
              "      <td>5</td>\n",
              "      <td>0.679810</td>\n",
              "      <td>0.679832</td>\n",
              "      <td>0.000595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.00001</td>\n",
              "      <td>6</td>\n",
              "      <td>0.678881</td>\n",
              "      <td>0.679764</td>\n",
              "      <td>0.000596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.00001</td>\n",
              "      <td>7</td>\n",
              "      <td>0.678023</td>\n",
              "      <td>0.679903</td>\n",
              "      <td>0.000596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.00001</td>\n",
              "      <td>8</td>\n",
              "      <td>0.677177</td>\n",
              "      <td>0.678742</td>\n",
              "      <td>0.000595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.00001</td>\n",
              "      <td>9</td>\n",
              "      <td>0.676340</td>\n",
              "      <td>0.676318</td>\n",
              "      <td>0.000594</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sparsity_lambda  epoch  val_pred_loss  train_pred_loss  \\\n",
              "0           0.01000      0       0.688561         0.688144   \n",
              "1           0.01000      1       0.685777         0.685254   \n",
              "2           0.01000      2       0.683678         0.681928   \n",
              "3           0.01000      3       0.682117         0.684018   \n",
              "4           0.01000      4       0.680886         0.683521   \n",
              "5           0.01000      5       0.679893         0.680334   \n",
              "6           0.01000      6       0.679038         0.680134   \n",
              "7           0.01000      7       0.678264         0.679861   \n",
              "8           0.01000      8       0.677501         0.677641   \n",
              "9           0.01000      9       0.676726         0.678914   \n",
              "10          0.00100      0       0.688556         0.688894   \n",
              "11          0.00100      1       0.685788         0.685571   \n",
              "12          0.00100      2       0.683697         0.683745   \n",
              "13          0.00100      3       0.682122         0.683165   \n",
              "14          0.00100      4       0.680895         0.682601   \n",
              "15          0.00100      5       0.679934         0.682531   \n",
              "16          0.00100      6       0.679097         0.680332   \n",
              "17          0.00100      7       0.678308         0.679547   \n",
              "18          0.00100      8       0.677521         0.679869   \n",
              "19          0.00100      9       0.676719         0.677866   \n",
              "20          0.00010      0       0.688564         0.686957   \n",
              "21          0.00010      1       0.685796         0.686587   \n",
              "22          0.00010      2       0.683633         0.684558   \n",
              "23          0.00010      3       0.681989         0.682575   \n",
              "24          0.00010      4       0.680755         0.680686   \n",
              "25          0.00010      5       0.679801         0.680708   \n",
              "26          0.00010      6       0.678986         0.681541   \n",
              "27          0.00010      7       0.678227         0.680874   \n",
              "28          0.00010      8       0.677469         0.679508   \n",
              "29          0.00010      9       0.676701         0.679961   \n",
              "30          0.00001      0       0.688676         0.686500   \n",
              "31          0.00001      1       0.685974         0.684515   \n",
              "32          0.00001      2       0.683793         0.683916   \n",
              "33          0.00001      3       0.682112         0.681476   \n",
              "34          0.00001      4       0.680840         0.680068   \n",
              "35          0.00001      5       0.679810         0.679832   \n",
              "36          0.00001      6       0.678881         0.679764   \n",
              "37          0.00001      7       0.678023         0.679903   \n",
              "38          0.00001      8       0.677177         0.678742   \n",
              "39          0.00001      9       0.676340         0.676318   \n",
              "\n",
              "    train_sparsity_loss  \n",
              "0              0.000570  \n",
              "1              0.000577  \n",
              "2              0.000584  \n",
              "3              0.000589  \n",
              "4              0.000593  \n",
              "5              0.000595  \n",
              "6              0.000596  \n",
              "7              0.000596  \n",
              "8              0.000595  \n",
              "9              0.000594  \n",
              "10             0.000569  \n",
              "11             0.000577  \n",
              "12             0.000583  \n",
              "13             0.000588  \n",
              "14             0.000592  \n",
              "15             0.000594  \n",
              "16             0.000595  \n",
              "17             0.000594  \n",
              "18             0.000593  \n",
              "19             0.000591  \n",
              "20             0.000570  \n",
              "21             0.000578  \n",
              "22             0.000585  \n",
              "23             0.000590  \n",
              "24             0.000595  \n",
              "25             0.000597  \n",
              "26             0.000598  \n",
              "27             0.000598  \n",
              "28             0.000597  \n",
              "29             0.000595  \n",
              "30             0.000570  \n",
              "31             0.000577  \n",
              "32             0.000584  \n",
              "33             0.000590  \n",
              "34             0.000594  \n",
              "35             0.000595  \n",
              "36             0.000596  \n",
              "37             0.000596  \n",
              "38             0.000595  \n",
              "39             0.000594  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train some models using the standard code\n",
        "\n",
        "from NM_TinyRNN.code.models import training\n",
        "from NM_TinyRNN.code.models import datasets\n",
        "from NM_TinyRNN.code.models import rnns\n",
        "from importlib import reload\n",
        "reload(training)\n",
        "reload(datasets)\n",
        "SAVE_PATH = CODE_DIR/'NM_TinyRNN/data/rnns'\n",
        "SPARSITY_LAMBDA = 0.0001\n",
        "DATA_PATH = './NM_TinyRNN/data/AB_behaviour/WS16'\n",
        "SEQUENCE_LENGTH = 150+1 # Define your desired sequence length\n",
        "dataset = datasets.AB_Dataset(DATA_PATH, SEQUENCE_LENGTH)\n",
        "model = rnns.TinyRNN(input_size=3, # past choice, past outcome, past forced choice\n",
        "                     hidden_size=1, # hidden unit\n",
        "                     out_size=2, # one-hot code for choice A, choice B\n",
        "                     rnn_type='GRU', # GRU, LSTM, NMRNN\n",
        "                sparsity_lambda = SPARSITY_LAMBDA) # Example sizes\n",
        "trainer = training.Trainer(SAVE_PATH/'GRU')\n",
        "trainer.fit(model,dataset)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
