{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charlesdgburns/NM_TinyRNN/blob/main/notebooks/nm_tinyrnn_dynamics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5W-ZRWU03M9"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Let's use this as a learning notebook about different RNN architectures.\n",
        "We want to start from scratch and get an idea of how different architectures work before fitting to some data.\n",
        "\n",
        "\n",
        "We the fit these to sequential behavioural decision making later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaPBpmAS_mMk",
        "outputId": "083a4bcf-4ae7-44f0-999e-8537c7c86660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NM_TinyRNN'...\n",
            "remote: Enumerating objects: 104, done.\u001b[K\n",
            "remote: Counting objects: 100% (104/104), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 104 (delta 37), reused 55 (delta 17), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (104/104), 212.00 KiB | 4.93 MiB/s, done.\n",
            "Resolving deltas: 100% (37/37), done.\n"
          ]
        }
      ],
      "source": [
        "## setup on google colab:\n",
        "\n",
        "!git clone https://github.com/charlesdgburns/NM_TinyRNN.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XNzAHyfqAuHq"
      },
      "outputs": [],
      "source": [
        "# setup\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBVkup08mkxr"
      },
      "source": [
        "# Recurrent Neural Network (RNN)\n",
        "\n",
        "The simplest of all recurrent architectures, which we think of as updating a hidden state $h_t$ using sensory inputs $x_{t-1}$ and recurrent inputs from the last hidden state $h_{t-1}$. Formally, this is described as a single discretised update:\n",
        "\n",
        "$$ h_t = \\tanh(W_{ih} x_{t-1} + b_{i} + W_{hh} h_{t-1} +b_{hh}) $$\n",
        "\n",
        "where $W_{ih}$ are the weights from the input to the hidden state, $W_{hh}$ are the recurrent weights from the hidden state to itself, and we consider additional biases $b_{i}$ and $b_{hh}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psieepNUAqte"
      },
      "outputs": [],
      "source": [
        "# let's do this later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-UbowrBmpK7"
      },
      "source": [
        "# Gated Recurrent Unit (GRU)\n",
        "\n",
        "We are taking inspiration from the following blogpost:\n",
        "https://medium.com/data-science/building-a-lstm-by-hand-on-pytorch-59c02a4ec091\n",
        "\n",
        "**The information flow in a gated recurrent unit**\n",
        "\n",
        "A gated recurrent unit has information flowing from the inputs $^{(1)}$ $x_{t-1}$  and the past hidden $h_{t-1}$, which is gated via 'reset' and 'update' gates $r_t$ and $z_t$ before giving the final output $h_t$. The update gate decides whether or not to overwrite a long-term memory with inputs, while the\n",
        "\n",
        "The gated recurrent unit allows the recurrent unit to persist its state and ignore its inputs.\n",
        "\n",
        "\n",
        "$$ r_t = \\sigma(W_{ir}x_{t-1}+b_{ir}+W_{hr}h_{t-1}+b_{hr}) \\qquad \\text{(reset)}$$\n",
        "\n",
        "$$ z_t = \\sigma(W_{iz}x_{t-1}+b_{iz}+W_{hz}h_{t-1}+b_{hz}) \\qquad\\text{(update)}$$\n",
        "\n",
        "$$ n_t = \\tanh(W_{in}x_{t-1}+b_{in}+r_{t}\\odot (W_{hn}h_{t-1}+b_{hn})) \\qquad\\text{(new)} $$\n",
        "\n",
        "$$ h_t = (1-z_t)\\odot n_t +z_t \\odot h_{t-1} \\qquad\\text{(gated reccurent output)}$$\n",
        "\n",
        "*footnotes*\n",
        "(1): inputs at time $t$ is recent experience\n",
        "\n",
        "Note that GRU's are markovian - their current state ($h_t$) can be determined entirely from its previous state ($h_{t-1}$) and inputs ($x_{t-1}$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ky3ugmGf0fBW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de1a3c47-fb5a-48ec-8910-8de21b8f2e45"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 150, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ManualGRU(nn.Module):\n",
        "  '''Manual GRU coded to return gate activations dictionary\n",
        "  Returns: (hidden_sequence, h_past) if return_gate_activations=False,\n",
        "           (hidden_sequence, gate_activations) if -||- = True'''\n",
        "  def __init__(self,input_size,hidden_size):\n",
        "    super().__init__() #init nn.Module\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.sigmoid = torch.nn.Sigmoid()\n",
        "    self.tanh = torch.nn.Tanh()\n",
        "\n",
        "    self.W_from_in = nn.Parameter(torch.Tensor(input_size, hidden_size*3))\n",
        "    self.W_from_h = nn.Parameter(torch.Tensor(hidden_size, hidden_size*3))\n",
        "    self.bias = nn.Parameter(torch.Tensor(hidden_size*6))\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "    for weight in self.parameters():\n",
        "        weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "  def forward(self, inputs, init_states = None,\n",
        "              return_gate_activations=False):\n",
        "    ''' inputs are a tensor of shape (batch_size, sequence_size, input_size)\n",
        "        outputs are tensor of shape (batch_size, sequence_size, hidden_size)'''\n",
        "\n",
        "    batch_size, sequence_size, _ = inputs.shape\n",
        "    hidden_sequence = []\n",
        "    if return_gate_activations:\n",
        "      gate_activations = {'reset':[],'update':[]}\n",
        "    if init_states is None:\n",
        "      h_past = torch.zeros(batch_size, self.hidden_size).to(inputs.device) #(n_hidden,batch_size)\n",
        "    else:\n",
        "      h_past = init_states\n",
        "\n",
        "    for t in range(sequence_size):\n",
        "      x_past = inputs[:,t,:] #(n_batch,input_size)\n",
        "      #for computational efficiency we do two matrix multiplications and then do indexing further down:\n",
        "      from_input = x_past@self.W_from_in + self.bias[:3*self.hidden_size]\n",
        "      from_input = from_input.view(batch_size,3, self.hidden_size) #(n_batch,3,n_hidden)\n",
        "      from_hidden = h_past@self.W_from_h + self.bias[3*self.hidden_size:]\n",
        "      from_hidden = from_hidden.view(batch_size, 3, self.hidden_size) #(n_batch,3, n_hidden)\n",
        "      r_t = self.sigmoid(from_input[:,0]+from_hidden[:,0]) #(n_batch,n_hidden), ranging from 0 to 1\n",
        "      z_t = self.sigmoid(from_input[:,1]+from_hidden[:,1]) #(n_batch,n_hidden), ranging from 0 to 1; must have n_hidden because it is multiplied with hidden_state later.\n",
        "      if return_gate_activations:\n",
        "        gate_activations['reset'].append(r_t.unsqueeze(0))\n",
        "        gate_activations['update'].append(z_t.unsqueeze(0))\n",
        "      n_t = self.tanh(from_input[:,2]+r_t*(from_hidden[:,2])).view(batch_size, self.hidden_size) #(n_batch,n_hidden)\n",
        "      h_past = (1-z_t)*n_t + z_t*h_past #(n_batch,hidden_size) #NOTE h_past is tehnically h_t now, but in the next for-loop it will be h_past. ;)\n",
        "      hidden_sequence.append(h_past.unsqueeze(0)) #appending (1,n_batch,n_hidden) to a big list.\n",
        "\n",
        "    hidden_sequence = torch.cat(hidden_sequence, dim=0) #(n_sequence, n_batch, n_hidden) gather all inputs along the first dimenstion\n",
        "    hidden_sequence = hidden_sequence.transpose(0, 1).contiguous() #reshape to batch first (n_batch,n_seq,n_hidden)\n",
        "\n",
        "    if return_gate_activations:\n",
        "        for gate_label, activations in gate_activations.items():\n",
        "            gate_activations[gate_label] = torch.cat(activations, dim=0).transpose(0,1).contiguous() #(n_batch,n_seq,n_hidden)\n",
        "        return hidden_sequence, gate_activations\n",
        "    else:\n",
        "      return hidden_sequence, h_past #this is standard in Pytorch, to output sequence of hidden states alongside most recent hidden state.\n",
        "\n",
        "test_inputs = torch.ones((8,150,3))\n",
        "test = ManualGRU(3, 2)\n",
        "hidden_sequence, h_past = test(test_inputs)\n",
        "hidden_sequence, gate_activations = test(test_inputs,return_gate_activations=True)\n",
        "gate_activations['reset'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVOFZkNDm9zE"
      },
      "source": [
        "# Long-Short-Term-Memory\n",
        "\n",
        "Here we implement a standard LSTM layer by layer.\n",
        "\n",
        "- Following https://medium.com/data-science/building-a-lstm-by-hand-on-pytorch-59c02a4ec091\n",
        "\n",
        "### 1. Forge gate\n",
        "\n",
        "The forget gate is where input, $x_t$, is combined with long term memory (termed the 'candidate'), $C_{t-1}$, and short term memory (the hidden state), $h_{t-1}$ . That is:\n",
        "\n",
        "$$ f_t =\\sigma(U_f x_t+V_f h_{t-1}+b_f),\\qquad C'_t=f_t\\cdot C_{t-1}$$\n",
        "\n",
        "where $\\sigma\\mapsto (0,1)$ is a sigmoidal activation function. The term $C'_t$ represents how much of the long term memory is retrieved.\n",
        "\n",
        "### 2. Input gate\n",
        "\n",
        "Where input informaion and hidden states are combined along with partial candidate $C'_t$ to form a new candidate:\n",
        "\n",
        "$$ i_t = \\sigma(U_ix_t+V_ih_{t-1}+b_i) \\\\\n",
        "C_t^+=\\tanh(U_c x_t+V_c h_{t-1} +b_c) \\\\\n",
        "C_t= C'_t+i_t\\cdot C_t^+$$\n",
        "\n",
        "Note here that $\\tanh\\mapsto (-1,1)$ affects how much information will be introduced to the memory.\n",
        "\n",
        "### 3. Output gate and hidden state of the cell\n",
        "\n",
        "Lastly we combine the l short term memory ($h_{t-1}$) and inputs ($x_t$) at the output gate, which is finally combined with long term memory (C_t) to get out final output ($h_t$).\n",
        "\n",
        "$$o_t=\\sigma(U_o  x_t+V_o h_{t-1}+b_o) \\\\ h_t = o_t\\cdot \\tanh(C_t) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IcmaEa-A-gFc"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class ManualLSTM(nn.Module):\n",
        "    def __init__(self, input_sz, hidden_sz, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.device=device\n",
        "        self.input_sz = input_sz\n",
        "        self.hidden_size = hidden_sz\n",
        "        self.W = nn.Parameter(torch.Tensor(input_sz, hidden_sz * 4))\n",
        "        self.U = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz * 4))\n",
        "        self.bias = nn.Parameter(torch.Tensor(hidden_sz * 4))\n",
        "        self.init_weights()\n",
        "\n",
        "\n",
        "    def init_weights(self):\n",
        "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv).to(self.device)\n",
        "\n",
        "    def forward(self, x, init_states=None,\n",
        "                return_gate_activations = False):\n",
        "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
        "        bs, seq_sz, _ = x.size() #let's get our shapes right\n",
        "        HS = self.hidden_size\n",
        "        if init_states is None:\n",
        "            h_t, c_t = (torch.zeros(bs, self.hidden_size).to(x.device),\n",
        "                        torch.zeros(bs, self.hidden_size).to(x.device))\n",
        "        else:\n",
        "            h_t, c_t = init_states\n",
        "\n",
        "        hidden_sequence = []#initialise lists for unit activations\n",
        "        if return_gate_activations:\n",
        "            gate_activations = {'input':[],'forget':[],'output':[], 'candidate':[]}\n",
        "        for t in range(seq_sz):\n",
        "            x_t = x[:, t, :]\n",
        "            # batch the computations into a single matrix multiplication\n",
        "            gates = x_t @ self.W + h_t @ self.U + self.bias\n",
        "            i_t, f_t, g_t, o_t = (\n",
        "                torch.sigmoid(gates[:, :HS]), # input\n",
        "                torch.sigmoid(gates[:, HS:HS*2]), # forget\n",
        "                torch.tanh(gates[:, HS*2:HS*3]), # this integrates inputs and past hidden (so over short-term memory)\n",
        "                torch.sigmoid(gates[:, HS*3:]), # output\n",
        "            )\n",
        "            c_t = f_t * c_t + i_t * g_t #here we integrate inputs/STM with long-term memory// never update candidate if forget = 1 and input = 0.\n",
        "            h_t = o_t * torch.tanh(c_t)\n",
        "            hidden_sequence.append(h_t.unsqueeze(0))\n",
        "            if return_gate_activations:\n",
        "                for gate_label, activation in {'input':i_t,'forget':f_t,'output':o_t,'candidate':c_t}.items():\n",
        "                    gate_activations[gate_label].append(activation.unsqueeze(0))\n",
        "        hidden_sequence = torch.cat(hidden_sequence, dim=0) #(sequence, batch, n_hidden)\n",
        "        hidden_sequence = hidden_sequence.transpose(0, 1).contiguous() #(batch, sequence, feature)\n",
        "        if return_gate_activations:\n",
        "            for gate_label, activations in gate_activations.items():\n",
        "                gate_activations[gate_label] = torch.cat(activations, dim=0).transpose(0,1).contiguous() #(n_batch,n_seq,n_hidden)\n",
        "            return hidden_sequence, gate_activations\n",
        "        else:\n",
        "            return hidden_sequence, (h_t, c_t)\n",
        "\n",
        "test_inputs = torch.ones((8,10,3))\n",
        "testLSTM = ManualLSTM(3,2)\n",
        "hidden_seq, hidden_tuple = testLSTM(test_inputs)\n",
        "hidden_seq, hidden_tuple = testLSTM(test_inputs, init_states = hidden_tuple)\n",
        "hidden_seq, gate_activations = testLSTM(test_inputs,\n",
        "                                        return_gate_activations = True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK6-Am2hOMf8"
      },
      "source": [
        "# NM-RNN\n",
        "\n",
        "[Costacura et al. (2024)](https://openreview.net/pdf?id=HbIBqn3grD) introduced NM-RNNs to bridge a gap between more standard RNNs today and biophysical models.\n",
        "\n",
        "Here we rewrite the equations (1) to (4) in Costacura et al. (2024) more similarly to the standard RNN notation above.\n",
        "\n",
        "We have inputs $x_{t-1}$ and our past hidden state $h_{t-1}$ which we want to integrate to get a new hidden state $h_t$. However, we want to selectively change (by a gain) the weights of our recurrent network depending on a neuromodulation signal $s(z(t))$.\n",
        "\n",
        "We therefore have a coupled network system, starting from a subnetwork state $z(t)$. In discretised terms:\n",
        "$$ \\tau_{z}  z_t = W_{zz} \\phi(z_t)+W_{iz} x_{t-1} \\qquad (1) $$\n",
        "\n",
        "$$ \\tau_{x} h_{t-1} = W_x(z_t)\\cdot\\phi(h_{t-1})+W_{ih} \\qquad (2)$$\n",
        "\n",
        "$$s(z(t)) = \\sigma(W_{zk} z_t+ b_k) \\qquad W_x(z_t)=\\sum_{k=1}^K s_k(z_t)\\mathcal{l}_k r_k^T \\qquad (4)$$\n",
        "\n",
        "Note that instead of a low-rank recurrent weight component, we want a tiny RNN, so we could modulating the weights associated with a given unit. Now the 'dynamic modes' are not the low ranks of a large network, but the activity of single units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xk47vkMkWps9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "6e7cb7e8-9606-4ae9-d4c1-c3b73c5be87b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1490030387.py, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1490030387.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    def __init__(self,input_size,hidden_size, nm_size batch_first = False):\u001b[0m\n\u001b[0m                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class ManualNMRNN(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size, nm_size batch_first = False):\n",
        "    super().__init__() #init nn.Module\n",
        "    self.sigmoid = torch.nn.Sigmoid()\n",
        "    self.tanh = torch.nn.Tanh()\n",
        "\n",
        "    self.W_from_in = nn.Parameter(torch.Tensor(input_size, hidden_size*2))\n",
        "    self.W_from_h = nn.Parameter(torch.Tensor(hidden_size, hidden_size*3))\n",
        "    self.W_from_nm = nn.Parameter(torch.Tensor(nm_size,hidden_size)) #not sure how to set up these parameters cleverly\n",
        "    self.bias = nn.Parameter(torch.Tensor(hidden_size*6))\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "    for weight in self.parameters():\n",
        "        weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "  def forward(self, inputs, init_states = None):\n",
        "    ''' inputs are a tensor of shape (batch_size, sequence_size, input_size)\n",
        "        outputs are tensor of shape (batch_size, sequence_size, hidden_size)'''\n",
        "\n",
        "    batch_size, sequence_size, _ = inputs.size\n",
        "    hidden_sequence = []\n",
        "    if init_states is None:\n",
        "      h_past = torch.zeros(batch_size, self.hidden_size).to(inputs.device)\n",
        "    else:\n",
        "      h_past = init_states\n",
        "\n",
        "    for t in range(sequence_size):\n",
        "      x_past = inputs[:,t,:] #(n_batch,input_size)\n",
        "      #for computational efficiency we do two matrix multiplications and then do indexing further down:\n",
        "      from_input = x_past@self.W_from_in + self.bias[:3]  #(n_batch,n_hidden)\n",
        "      from_hidden = h_past@self.W_from_h + self.bias[3:]  #(n_batch,n_hidden)\n",
        "\n",
        "      r_t =self.sigmoid(from_input[0]+from_hidden[0]) #(n_batch,n_hidden), ranging from 0 to 1\n",
        "      z_t = self.sigmoid(from_input[1]+from_hidden[1]) #(n_batch,n_hidden), ranging from 0 to 1\n",
        "      n_t = self.tanh(from_input[2]+r_t*(from_hidden[2])) #(n_batch,n_hidden)\n",
        "      h_past = (1-z_t)*n_t + z_t*h_past #(n_batch,hidden_size) #NOTE h_past is tehnically h_t now, but in the next for-loop it will be h_past. ;)\n",
        "      hidden_sequence.append(h_past.unsqueeze(0)) #appending (1,n_batch,n_hidden) to a big list.\n",
        "    hidden_sequence = torch.cat(hidden_sequence, dim=0) #(n_sequence, n_batch, n_hidden) gather all inputs along the first dimenstion\n",
        "    hidden_sequence = hidden_sequence.transpose(0, 1).contiguous() #reshape to batch first (n_batch,n_seq,n_hidden)\n",
        "    return hidden_sequence, h_past #this is standard in Pytorch, to output sequence of hidden states alongside most recent hidden state.\n",
        "\n",
        "\n",
        "test_inputs = torch.ones((8,10,3))\n",
        "testLSTM = ManualLSTM(3,2)\n",
        "hidden_seq, hidden_tuple = testLSTM(test_inputs)\n",
        "hidden_seq, hidden_tuple = testLSTM(test_inputs, init_states = hidden_tuple)\n",
        "hidden_seq, gate_activations = testLSTM(test_inputs,\n",
        "                                        return_gate_activations = True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCIgC3YoBTW-"
      },
      "source": [
        "### Low-rank decomposition\n",
        "\n",
        "The reason why an NM-RNN scales the low-rank components of the weight matrix, is that it allows it to selectively scale the different dimensions of the recurrent update.\n",
        "\n",
        "Wait a second - what is a low-rank decomposition?\n",
        "Let's take a look at one.\n",
        "\n",
        "We have an example of a very simple 2-unit network with different recurrent dynamics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__Ca3FZ8NcOo"
      },
      "outputs": [],
      "source": [
        "# Potentially useful examples of recurrent weights affecting dynamics\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------- Parameter sets (from your MATLAB) ----------\n",
        "single = {\n",
        "    \"I_y\": np.array([-10, -10]),\n",
        "    \"I_t1\": np.array([0, 20]),\n",
        "    \"I_t2\": np.array([20, 0]),\n",
        "    \"W\": np.array([[-0.2, 0.8],\n",
        "                   [ 0.8,-0.2]]),\n",
        "    \"r_init\": np.array([50, 50]),\n",
        "}\n",
        "\n",
        "multi = {\n",
        "    \"I_y\": np.array([-10, -5]),\n",
        "    \"I_t1\": np.array([0, 20]),\n",
        "    \"I_t2\": np.array([-20, 0]),\n",
        "    \"W\": np.array([[ 1.1,-0.15],\n",
        "                   [-0.15, 1.1]]),\n",
        "    \"r_init\": np.array([50, 50]),\n",
        "}\n",
        "\n",
        "continuous = {\n",
        "    \"I_y\": np.array([-10, -10]),\n",
        "    \"I_t1\": np.array([0.5, 0.0]),\n",
        "    \"I_t2\": np.array([0.5, 0.0]),\n",
        "    \"W\": np.array([[ 0.8,-0.2],\n",
        "                   [-0.2, 0.8]]),\n",
        "    \"r_init\": np.array([50, 50]),\n",
        "}\n",
        "\n",
        "oscillator = {\n",
        "    \"I_y\": np.array([ 8, 20]),\n",
        "    \"I_t1\": np.array([0, 0]),\n",
        "    \"I_t2\": np.array([0,10]),\n",
        "    \"W\": np.array([[ 2.2,-1.3],\n",
        "                   [ 1.2,-0.1]]),\n",
        "    \"r_init\": np.array([50, 50]),\n",
        "}\n",
        "\n",
        "# --------- choose your configuration (match MATLAB: p = continuous) ----------\n",
        "p = oscillator  # or: single, multi, oscillator\n",
        "\n",
        "# ---------- Simulation settings (match MATLAB) ----------\n",
        "dt = 1              # ms\n",
        "t_max = 3000        # ms\n",
        "duration = 250      # ms (length of each stimulation)\n",
        "t1 = 1000           # ms\n",
        "t2 = 2000           # ms\n",
        "dt_tau = 0.1        # Euler ratio for 1 ms step and tau = 10 ms\n",
        "\n",
        "# ---------- Build inputs over time (shape: T+1 by 2) ----------\n",
        "T = int(t_max / dt)\n",
        "times = np.arange(0, T + 1)  # 0..3000 inclusive -> 3001 samples\n",
        "inputs = np.zeros((T + 1, 2))\n",
        "\n",
        "idx1 = int(t1 / dt)\n",
        "idx2 = int(t2 / dt)\n",
        "dur = int(duration / dt)\n",
        "\n",
        "inputs[idx1:idx1 + dur, :] = p[\"I_t1\"]\n",
        "inputs[idx2:idx2 + dur, :] = p[\"I_t2\"]\n",
        "\n",
        "# ---------- Fire-rate simulation (two units, linear-threshold with clipping) ----------\n",
        "N = np.zeros((T + 1, 2))\n",
        "N[0, :] = p[\"r_init\"]\n",
        "\n",
        "W = p[\"W\"]\n",
        "I_y = p[\"I_y\"]\n",
        "\n",
        "for k in range(T):  # update N[k+1] from N[k]\n",
        "    # neuron 1\n",
        "    N[k+1, 0] = N[k, 0] + dt_tau * (-N[k, 0] + W[0, :] @ N[k, :] + inputs[k, 0] - I_y[0])\n",
        "    # neuron 2\n",
        "    N[k+1, 1] = N[k, 1] + dt_tau * (-N[k, 1] + W[1, :] @ N[k, :] + inputs[k, 1] - I_y[1])\n",
        "\n",
        "    # rectify to [0, 100] Hz\n",
        "    N[k+1, :] = np.clip(N[k+1, :], 0.0, 100.0)\n",
        "\n",
        "# ---------- Dynamics classification (same formulas as your MATLAB snippet) ----------\n",
        "trace = -2 + W[0,0] + W[1,1]\n",
        "det = (-1 + W[0,0]) * (-1 + W[1,1]) - W[0,1] * W[1,0]\n",
        "\n",
        "stability = \"unstable\" if trace > 0 else \"stable\"\n",
        "node_or_focus = \"node\" if trace**2 > 4 * det else \"focus\"\n",
        "\n",
        "print(f\"trace = {trace:.3f}\")\n",
        "print(f\"det   = {det:.3f}\")\n",
        "print(f\"Stability: {stability}\")\n",
        "print(f\"Type: {node_or_focus}\")\n",
        "\n",
        "# ---------- Plot (3 panels similar to MATLAB layout) ----------\n",
        "n1_colour = (0.5, 0.7, 1.0, 0.3)\n",
        "n2_colour = (1.0, 0.4, 0.4, 0.3)\n",
        "\n",
        "fig = plt.figure(figsize=(9, 5))\n",
        "gs = fig.add_gridspec(3, 5)\n",
        "\n",
        "# Firing rates over time: subplot(3,5,[6 7]) -> row=1, cols=0:2\n",
        "ax_rates = fig.add_subplot(gs[1, 0:2])\n",
        "ax_rates.plot(times, N[:, 0], color=n1_colour)\n",
        "ax_rates.plot(times, N[:, 1], color=n2_colour)\n",
        "ax_rates.set_xlim(0, T)\n",
        "ax_rates.set_ylabel(\"Firing rate (Hz)\")\n",
        "ax_rates.set_title(\"Firing rates\")\n",
        "\n",
        "# Inputs over time: subplot(3,5,[11 12]) -> row=2, cols=0:2\n",
        "ax_inputs = fig.add_subplot(gs[2, 0:2])\n",
        "ax_inputs.plot(times, inputs[:, 0], color=n1_colour)\n",
        "ax_inputs.plot(times, inputs[:, 1], color=n2_colour)\n",
        "ax_inputs.set_xlim(0, T)\n",
        "ax_inputs.set_ylabel(\"Injected currents\")\n",
        "ax_inputs.set_xlabel(\"Time (ms)\")\n",
        "ax_inputs.set_title(\"Inputs\")\n",
        "\n",
        "# Phase-plane: subplot(3,5,[4 5 9 10 14 15]) -> all rows, cols=3:5 (right block)\n",
        "ax_phase = fig.add_subplot(gs[:, 3:5])\n",
        "c = np.linspace(1, 10, N.shape[0])\n",
        "ax_phase.scatter(N[:, 0], N[:, 1], s=5, c=c)\n",
        "ax_phase.scatter(N[0, 0], N[0, 1], s=50, c='k', marker='+', label='start')\n",
        "ax_phase.scatter(N[-1, 0], N[-1, 1], s=25, c='k', label='end')\n",
        "ax_phase.set_xlim(0, 100)\n",
        "ax_phase.set_ylim(0, 100)\n",
        "ax_phase.set_xlabel(\"Neuron 1 Hz\")\n",
        "ax_phase.set_ylabel(\"Neuron 2 Hz\")\n",
        "ax_phase.set_aspect('equal', adjustable='box')\n",
        "ax_phase.set_title(\"Phase-plane\")\n",
        "ax_phase.legend(loc=\"best\", frameon=False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "340ROps8RdNv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRkxKOUVg3Ec"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I3UnSzOg3mz"
      },
      "source": [
        "# Fitting to behaviour\n",
        "\n",
        "We have behavioral data from a two-armed-bandit reversal task.\n",
        "\n",
        "Mice are presented with two cue lights (A and B) and need to make a choice. Probabilities for rewards are 0.75 ('good poke') for one state and 0.25 on the other (not 'good poke'), a state which reverses randomly. However, occasionally only one light goes on (A or B) and the mouse must make their next poke there to continue the task, so this is considered 'forced_choice'.\n",
        "\n",
        "Let's load some data:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhzsatwriXep"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = Path('./NM_TinyRNN/data/')\n",
        "TASK = 'AB_behaviour'\n",
        "\n",
        "subject_data = []\n",
        "for subject_dir in (DATA_PATH/TASK).iterdir():\n",
        "  if subject_dir.is_dir(): #skip the README file\n",
        "    for session_dir in subject_dir.iterdir():\n",
        "      if 'WS16' in session_dir.stem: #theres duplicate data with weird names/skip this\n",
        "        continue\n",
        "      if session_dir.is_dir(): #skip the .DS_session stuff\n",
        "        subject_data.append(pd.read_csv(session_dir/'trials.htsv', sep = '\\t'))\n",
        "subject_df = pd.concat(subject_data) #concatenate across sessions here (that's ok)\n",
        "\n",
        "subject_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4E3266iitDn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "class AB_Dataset(Dataset):\n",
        "    def __init__(self, subject_data_path, sequence_length):\n",
        "        self.subject_data_path = Path(subject_data_path)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.subject_df = self._load_and_concat_data()\n",
        "        self.inputs, self.targets = self._create_sequences()\n",
        "\n",
        "    def _load_and_concat_data(self):\n",
        "        subject_data = []\n",
        "        for session_dir in self.subject_data_path.iterdir():\n",
        "            if 'WS16' in session_dir.stem:\n",
        "                continue\n",
        "            if session_dir.is_dir():\n",
        "                subject_data.append(pd.read_csv(session_dir/'trials.htsv', sep = '\\t'))\n",
        "        return pd.concat(subject_data)\n",
        "\n",
        "    def _create_sequences(self):\n",
        "        # Convert boolean and categorical columns to numerical\n",
        "        df_processed = self.subject_df.copy()\n",
        "        df_processed['forced_choice'] = df_processed['forced_choice'].astype(int)\n",
        "        df_processed['outcome'] = df_processed['outcome'].astype(int)\n",
        "        df_processed['choice'] = df_processed['choice'].astype('category').cat.codes\n",
        "\n",
        "        # Convert to tensor and handle potential remainder\n",
        "        data_tensor = torch.tensor(df_processed[['forced_choice', 'outcome', 'choice']].values, dtype=torch.float32)\n",
        "        num_rows = data_tensor.size(0)\n",
        "        remainder = num_rows % self.sequence_length\n",
        "        if remainder != 0:\n",
        "            data_tensor = data_tensor[:-remainder] # Trim off the remainder\n",
        "\n",
        "        # Reshape into sequences\n",
        "        num_sequences = data_tensor.size(0) // self.sequence_length\n",
        "        sequences = data_tensor.view(num_sequences, self.sequence_length, data_tensor.size(1))\n",
        "\n",
        "        # Create inputs and targets\n",
        "        # Inputs are 'forced_choice' and 'outcome' at time t\n",
        "        inputs = sequences[:, :-1, :]\n",
        "        # Targets are 'choice' at time t+1, one-hot encoded\n",
        "        targets_codes = sequences[:, 1:, 2].long() # Get the categorical codes as long tensor\n",
        "        targets = torch.nn.functional.one_hot(targets_codes, num_classes=2).float() # One-hot encode\n",
        "\n",
        "\n",
        "        return inputs, targets\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.inputs.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx]\n",
        "\n",
        "# Example usage:\n",
        "# Define the data path and sequence length\n",
        "DATA_PATH = './NM_TinyRNN/data/AB_behaviour/WS16'\n",
        "SEQUENCE_LENGTH = 150+1 # Define your desired sequence length\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "dataset = AB_Dataset(DATA_PATH, SEQUENCE_LENGTH)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Example of iterating through the dataloader\n",
        "# for batch_inputs, batch_targets in dataloader:\n",
        "#      print(\"Inputs shape:\", batch_inputs.shape)\n",
        "#      print(\"Targets shape:\", batch_targets.shape)\n",
        "#      break # Just printing the first batch shapes for demonstration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sc7lAhX4qqNi"
      },
      "outputs": [],
      "source": [
        "# Create a flexible way of training different RNNs to do the task prediction\n",
        "\n",
        "class TinyRNN(nn.Module):\n",
        "  def __init__(self, input_size:int=3, hidden_size:int=1, out_size:int=2,\n",
        "               rnn_type = 'GRU',\n",
        "               sparsity_lambda:float = 1e-2):\n",
        "    super().__init__()\n",
        "    self.I = input_size\n",
        "    self.H = hidden_size\n",
        "    self.O = out_size\n",
        "    self.rnn_type = rnn_type\n",
        "    if rnn_type == 'GRU':\n",
        "      self.rnn = ManualGRU(self.I,self.H)\n",
        "    elif rnn_type == 'LSTM':\n",
        "      self.rnn = ManualLSTM(self.I,self.H)\n",
        "    elif rnn_type == 'NMRNN':\n",
        "      self.rnn = ManualNMRNN(self.I,self.H)\n",
        "    self.decoder = nn.Linear(self.H, self.O)\n",
        "    self.sparsity_lambda = sparsity_lambda\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    hidden, _ = self.rnn(inputs)\n",
        "    predictions = self.decoder(hidden)\n",
        "    return predictions\n",
        "\n",
        "  def compute_losses(self, predictions, targets):\n",
        "    prediction_loss = nn.functional.cross_entropy(predictions, targets) #NB: this applies softmax itself\n",
        "    #for sparsity we need to select the right weights to regularise\n",
        "    sparsity_loss = 0 #init sparsity_loss\n",
        "    for name, param in self.rnn.named_parameters():\n",
        "      if 'bias' not in name:\n",
        "        sparsity_loss += self.sparsity_lambda*torch.abs(param).sum()\n",
        "    return prediction_loss, sparsity_loss\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUxNsKot7GaY"
      },
      "outputs": [],
      "source": [
        "from NM_TinyRNN.code.models import training\n",
        "from importlib import reload; reload(training);\n",
        "DATA_PATH = Path('./NM_TinyRNN/data/rnns')\n",
        "model = TinyRNN(input_size=3, hidden_size=1, out_size=2, rnn_type='GRU', sparsity_lambda = SPARSITY_LAMBDA) # Example sizes\n",
        "trainer = training.Trainer(DATA_PATH/'GRU')\n",
        "trainer.fit(model,dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35lrv5n-7GaY"
      },
      "outputs": [],
      "source": [
        "best_state_dict = torch.load(DATA_PATH/'GRU/1_unit_GRU_L1_0.0001_model_state.pth')\n",
        "model.load_state_dict(best_state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inle6uOO7GaY"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "trial_by_trial_data = {}\n",
        "n_batches, n_seq, n_inputs = dataset.inputs.shape\n",
        "inputs_reshaped = dataset.inputs.reshape(n_batches*n_seq,n_inputs).unsqueeze(0)\n",
        "with torch.no_grad():\n",
        "    predictions = model(inputs_reshaped)\n",
        "    hidden_state, gate_activations = model.rnn(inputs_reshaped, return_gate_activations = True)\n",
        "# add hidden activations\n",
        "for each_unit in range(model.H):\n",
        "    trial_by_trial_data[f'hidden_{each_unit+1}'] = hidden_state[:,:,each_unit].flatten()\n",
        "#add gate activations\n",
        "for gate_label, activations in gate_activations.items():\n",
        "    print(activations.shape)\n",
        "    for each_unit in range(activations.shape[-1]):\n",
        "        trial_by_trial_data[f'gate_{gate_label}_{each_unit+1}'] = activations[:,:,each_unit].flatten()\n",
        "# add logit and probabilities data:\n",
        "log_probs= predictions.log_softmax(dim=2)\n",
        "logits = (log_probs[:,:,0] - log_probs[:,:,1]).flatten()\n",
        "trial_by_trial_data['logit_value'] = logits\n",
        "trial_by_trial_data['logit_change'] = torch.cat([torch.tensor([torch.nan]),(logits[1:]-logits[:-1])])\n",
        "trial_by_trial_data['prob_A'] = torch.exp(log_probs[:,:,0]).flatten()\n",
        "trial_by_trial_data['prob_b'] = torch.exp(log_probs[:,:,1]).flatten()\n",
        "\n",
        "#add whether trial was used in training, validation, or evaluation\n",
        "_,_,_, indices_dict = trainer._split_dataset(dataset)\n",
        "for idx_label, idx_values in indices_dict.items():\n",
        "    # Generate all trial indices for the given batches\n",
        "    trial_indices = np.concatenate([np.arange(idx * n_seq, (idx + 1) * n_seq)\n",
        "                                    for idx in idx_values])\n",
        "    # Create boolean mask using isin\n",
        "    all_trial_indices = np.arange(n_batches * n_seq)\n",
        "    label_indices = np.isin(all_trial_indices, trial_indices)\n",
        "    indices_dict[idx_label] = label_indices\n",
        "trial_by_trial_data.update(indices_dict)\n",
        "df = pd.DataFrame(trial_by_trial_data)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJwyvoa37GaY"
      },
      "outputs": [],
      "source": [
        "hidden_state, gate_activations = model.rnn(inputs_reshaped, return_gate_activations = True)\n",
        "gate_activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGLbKqj57GaY"
      },
      "outputs": [],
      "source": [
        "_, _, _, indices_dict = trainer._split_dataset(dataset)\n",
        "\n",
        "for idx_label, idx_values in indices_dict.items():\n",
        "    # Generate all trial indices for the given batches\n",
        "    trial_indices = np.concatenate([\n",
        "        np.arange(idx * n_seq, (idx + 1) * n_seq)\n",
        "        for idx in idx_values])\n",
        "\n",
        "    # Create boolean mask using isin\n",
        "    all_trial_indices = np.arange(n_batches * n_seq)\n",
        "    label_indices = np.isin(all_trial_indices, trial_indices)\n",
        "\n",
        "    indices_dict[idx_label] = label_indices\n",
        "indices_dict"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}