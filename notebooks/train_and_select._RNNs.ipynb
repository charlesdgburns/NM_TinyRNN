{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d84fa193",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Short notebook to get an overview of model performance across architectures and dimensionality.\n",
    "\n",
    "Here we share some code to train a model from scratch and our procedure for evaluating and selecting models to study.\n",
    "\n",
    "For each architecture (RNN, GRU, LSTM, NM-RNN) and dimensionality (1 or 2 hidden units) we fit a model to predict a subject's (WS16) choices during a 2-armed bandit task (75% reward probability at 'good poke' and 25% reward probability at the other poke). Note that in this data there are forced choice trials, but the loss associated with these predictions are excluded (since forced choices are random and cannot be predicted well).\n",
    "\n",
    "All trials for a subject are split into train (80%), validation (10%), an evaluation (10%), seeded deterministically across models and repeated for 10 different seeds. \n",
    "\n",
    "Training is done with an early stop on validation prediction errors within each model type but across hyperparameters (varying sparsity). \n",
    "\n",
    "Then across models we compare performance on evaluation prediction errors. \n",
    "\n",
    "We then select the median performing model on the evaluation performance, so as not to overfit the transfer learning variability across splits.\n",
    "\n",
    "We further study how these models predict the behaviour in the `mechanisms` notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup on google colab:\n",
    "\n",
    "!git clone https://github.com/charlesdgburns/NM_TinyRNN.git\n",
    "from pathlib import Path\n",
    "CODE_DIR = Path('.')\n",
    "SAVE_PATH = CODE_DIR/'NM_TinyRNN/data/rnns'\n",
    "DATA_PATH = './NM_TinyRNN/data/AB_behaviour/WS16'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f12b5746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general setup:\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from NM_TinyRNN.code.models import training\n",
    "from NM_TinyRNN.code.models import datasets\n",
    "from NM_TinyRNN.code.models import rnns\n",
    "from NM_TinyRNN.code.models import parallelised_training as pat\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6281c99",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "Here we leave a bit of code to show how we train models, first one-by-one and then with a parallelised training approach (on a SLURM managed computing cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d466cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 1 sparsity values...\n",
      "Dataset size: 51\n",
      "Split sizes - Train: 40, Validation: 5, Evaluation: 6\n",
      "\n",
      "Training with sparsity lambda = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "λ=1e-04:   4%|▍         | 423/10000 [04:07<1:33:22,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping after 424 epochs\n",
      "\n",
      "Evaluating best model (λ=1e-04) on test set...\n",
      "Evaluation loss: 0.495272\n",
      "\n",
      "Training complete!\n",
      "Best model (λ=1e-04) saved to: NM_TinyRNN/data/rnns/test\n",
      "Best validation loss: 0.526913\n",
      "Test loss: 0.495272\n",
      "Lastly, extracting activations on full dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_1</th>\n",
       "      <th>hidden_2</th>\n",
       "      <th>gate_subnetwork_1</th>\n",
       "      <th>gate_subnetwork_2</th>\n",
       "      <th>gate_modulation_1</th>\n",
       "      <th>gate_modulation_2</th>\n",
       "      <th>logit_value</th>\n",
       "      <th>logit_past</th>\n",
       "      <th>logit_change</th>\n",
       "      <th>prob_A</th>\n",
       "      <th>...</th>\n",
       "      <th>forced_choice</th>\n",
       "      <th>choice</th>\n",
       "      <th>outcome</th>\n",
       "      <th>good_poke</th>\n",
       "      <th>session_trial_idx</th>\n",
       "      <th>session_folder_name</th>\n",
       "      <th>trial_type</th>\n",
       "      <th>indices_train</th>\n",
       "      <th>indices_validation</th>\n",
       "      <th>indices_evaluation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.535300</td>\n",
       "      <td>-0.487991</td>\n",
       "      <td>-0.535177</td>\n",
       "      <td>0.581380</td>\n",
       "      <td>0.112868</td>\n",
       "      <td>0.430158</td>\n",
       "      <td>2.015045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.882368</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-01-164959</td>\n",
       "      <td>A1, R=1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.761752</td>\n",
       "      <td>-0.720843</td>\n",
       "      <td>-0.189683</td>\n",
       "      <td>0.751821</td>\n",
       "      <td>0.152007</td>\n",
       "      <td>0.457268</td>\n",
       "      <td>2.851305</td>\n",
       "      <td>2.015045</td>\n",
       "      <td>0.836260</td>\n",
       "      <td>0.945386</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-05-01-164959</td>\n",
       "      <td>A1, R=1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.993731</td>\n",
       "      <td>0.376883</td>\n",
       "      <td>0.708129</td>\n",
       "      <td>-0.541793</td>\n",
       "      <td>0.968871</td>\n",
       "      <td>0.860629</td>\n",
       "      <td>1.180477</td>\n",
       "      <td>2.851305</td>\n",
       "      <td>-1.670828</td>\n",
       "      <td>0.765034</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-05-01-164959</td>\n",
       "      <td>A1, R=0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.997339</td>\n",
       "      <td>0.982201</td>\n",
       "      <td>0.274605</td>\n",
       "      <td>-0.843544</td>\n",
       "      <td>0.961717</td>\n",
       "      <td>0.854337</td>\n",
       "      <td>0.041843</td>\n",
       "      <td>1.180477</td>\n",
       "      <td>-1.138634</td>\n",
       "      <td>0.510459</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-05-01-164959</td>\n",
       "      <td>A1, R=0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.039886</td>\n",
       "      <td>0.899150</td>\n",
       "      <td>-0.820585</td>\n",
       "      <td>0.507578</td>\n",
       "      <td>0.075190</td>\n",
       "      <td>0.392266</td>\n",
       "      <td>-1.474650</td>\n",
       "      <td>0.041843</td>\n",
       "      <td>-1.516493</td>\n",
       "      <td>0.186237</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-05-01-164959</td>\n",
       "      <td>A2, R=1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7835</th>\n",
       "      <td>-0.210884</td>\n",
       "      <td>0.782974</td>\n",
       "      <td>-0.516520</td>\n",
       "      <td>0.811097</td>\n",
       "      <td>0.070111</td>\n",
       "      <td>0.379432</td>\n",
       "      <td>-0.956011</td>\n",
       "      <td>1.246108</td>\n",
       "      <td>-2.202118</td>\n",
       "      <td>0.277678</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>444</td>\n",
       "      <td>2025-04-28-181702</td>\n",
       "      <td>A2, R=1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7836</th>\n",
       "      <td>-0.951468</td>\n",
       "      <td>0.988801</td>\n",
       "      <td>0.834836</td>\n",
       "      <td>-0.470577</td>\n",
       "      <td>0.971869</td>\n",
       "      <td>0.864392</td>\n",
       "      <td>-0.050820</td>\n",
       "      <td>-0.956011</td>\n",
       "      <td>0.905190</td>\n",
       "      <td>0.487298</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>445</td>\n",
       "      <td>2025-04-28-181702</td>\n",
       "      <td>A1, R=0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7837</th>\n",
       "      <td>0.004184</td>\n",
       "      <td>0.893274</td>\n",
       "      <td>-0.949323</td>\n",
       "      <td>0.498699</td>\n",
       "      <td>0.058883</td>\n",
       "      <td>0.369953</td>\n",
       "      <td>-1.540569</td>\n",
       "      <td>-0.050820</td>\n",
       "      <td>-1.489749</td>\n",
       "      <td>0.176453</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>446</td>\n",
       "      <td>2025-04-28-181702</td>\n",
       "      <td>A2, R=1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7838</th>\n",
       "      <td>-0.700327</td>\n",
       "      <td>0.999625</td>\n",
       "      <td>0.844589</td>\n",
       "      <td>-0.271939</td>\n",
       "      <td>0.955885</td>\n",
       "      <td>0.840895</td>\n",
       "      <td>-0.510278</td>\n",
       "      <td>-1.540569</td>\n",
       "      <td>1.030292</td>\n",
       "      <td>0.375128</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>447</td>\n",
       "      <td>2025-04-28-181702</td>\n",
       "      <td>A2, R=0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7839</th>\n",
       "      <td>-0.991602</td>\n",
       "      <td>0.996462</td>\n",
       "      <td>0.075281</td>\n",
       "      <td>-0.827049</td>\n",
       "      <td>0.939565</td>\n",
       "      <td>0.831011</td>\n",
       "      <td>0.004840</td>\n",
       "      <td>-0.510278</td>\n",
       "      <td>0.515117</td>\n",
       "      <td>0.501210</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>448</td>\n",
       "      <td>2025-04-28-181702</td>\n",
       "      <td>A1, R=0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7840 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      hidden_1  hidden_2  gate_subnetwork_1  gate_subnetwork_2  \\\n",
       "0    -0.535300 -0.487991          -0.535177           0.581380   \n",
       "1    -0.761752 -0.720843          -0.189683           0.751821   \n",
       "2    -0.993731  0.376883           0.708129          -0.541793   \n",
       "3    -0.997339  0.982201           0.274605          -0.843544   \n",
       "4    -0.039886  0.899150          -0.820585           0.507578   \n",
       "...        ...       ...                ...                ...   \n",
       "7835 -0.210884  0.782974          -0.516520           0.811097   \n",
       "7836 -0.951468  0.988801           0.834836          -0.470577   \n",
       "7837  0.004184  0.893274          -0.949323           0.498699   \n",
       "7838 -0.700327  0.999625           0.844589          -0.271939   \n",
       "7839 -0.991602  0.996462           0.075281          -0.827049   \n",
       "\n",
       "      gate_modulation_1  gate_modulation_2  logit_value  logit_past  \\\n",
       "0              0.112868           0.430158     2.015045         NaN   \n",
       "1              0.152007           0.457268     2.851305    2.015045   \n",
       "2              0.968871           0.860629     1.180477    2.851305   \n",
       "3              0.961717           0.854337     0.041843    1.180477   \n",
       "4              0.075190           0.392266    -1.474650    0.041843   \n",
       "...                 ...                ...          ...         ...   \n",
       "7835           0.070111           0.379432    -0.956011    1.246108   \n",
       "7836           0.971869           0.864392    -0.050820   -0.956011   \n",
       "7837           0.058883           0.369953    -1.540569   -0.050820   \n",
       "7838           0.955885           0.840895    -0.510278   -1.540569   \n",
       "7839           0.939565           0.831011     0.004840   -0.510278   \n",
       "\n",
       "      logit_change    prob_A  ...  forced_choice  choice  outcome  good_poke  \\\n",
       "0              NaN  0.882368  ...              1       0        1          1   \n",
       "1         0.836260  0.945386  ...              0       0        1          1   \n",
       "2        -1.670828  0.765034  ...              0       0        0          1   \n",
       "3        -1.138634  0.510459  ...              0       0        0          1   \n",
       "4        -1.516493  0.186237  ...              1       1        1          1   \n",
       "...            ...       ...  ...            ...     ...      ...        ...   \n",
       "7835     -2.202118  0.277678  ...              0       1        1          1   \n",
       "7836      0.905190  0.487298  ...              1       0        0          1   \n",
       "7837     -1.489749  0.176453  ...              0       1        1          1   \n",
       "7838      1.030292  0.375128  ...              0       1        0          1   \n",
       "7839      0.515117  0.501210  ...              1       0        0          1   \n",
       "\n",
       "      session_trial_idx  session_folder_name trial_type indices_train  \\\n",
       "0                     0    2025-05-01-164959    A1, R=1          True   \n",
       "1                     1    2025-05-01-164959    A1, R=1          True   \n",
       "2                     2    2025-05-01-164959    A1, R=0          True   \n",
       "3                     3    2025-05-01-164959    A1, R=0          True   \n",
       "4                     4    2025-05-01-164959    A2, R=1          True   \n",
       "...                 ...                  ...        ...           ...   \n",
       "7835                444    2025-04-28-181702    A2, R=1         False   \n",
       "7836                445    2025-04-28-181702    A1, R=0         False   \n",
       "7837                446    2025-04-28-181702    A2, R=1         False   \n",
       "7838                447    2025-04-28-181702    A2, R=0         False   \n",
       "7839                448    2025-04-28-181702    A1, R=0         False   \n",
       "\n",
       "      indices_validation  indices_evaluation  \n",
       "0                  False               False  \n",
       "1                  False               False  \n",
       "2                  False               False  \n",
       "3                  False               False  \n",
       "4                  False               False  \n",
       "...                  ...                 ...  \n",
       "7835               False               False  \n",
       "7836               False               False  \n",
       "7837               False               False  \n",
       "7838               False               False  \n",
       "7839               False               False  \n",
       "\n",
       "[7840 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train some models using the standard code\n",
    "reload(training)\n",
    "reload(datasets)\n",
    "import torch\n",
    "\n",
    "CODE_DIR = Path('.')\n",
    "SAVE_PATH = CODE_DIR/'NM_TinyRNN/data/rnns'\n",
    "DATA_PATH = './NM_TinyRNN/data/AB_behaviour/WS16'\n",
    "SEQUENCE_LENGTH = 150 # sequence length for batched training.\n",
    "\n",
    "random_seed = 5\n",
    "\n",
    "options_dict = {'rnn_type':'NMRNN',\n",
    "                'input_size':3,\n",
    "                'hidden_size':2,\n",
    "                'out_size':2,\n",
    "                'nm_size':2,\n",
    "                'nm_dim':2,\n",
    "                'nm_mode':'low_rank',\n",
    "                'seed':random_seed}\n",
    "\n",
    "dataset = datasets.AB_Dataset(DATA_PATH, SEQUENCE_LENGTH)\n",
    "model = rnns.TinyRNN(**options_dict)  \n",
    "\n",
    "trainer = training.Trainer(SAVE_PATH/'test',batch_size = 8, \n",
    "                                            max_epochs = 10000, \n",
    "                                            early_stop = 200,\n",
    "                                            random_seed = 24,\n",
    "                                            sparsity_lambda = [1e-4]\n",
    "                                            )\n",
    "training_losses_df = trainer.fit(model,dataset)\n",
    "# also get trials_df\n",
    "trials_df = trainer.get_model_trial_by_trial_df(model,dataset)\n",
    "trials_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324212b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293776\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293777\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293778\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293779\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293780\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293781\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293782\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293783\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293784\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293785\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293786\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293787\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293788\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293789\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293790\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293791\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293792\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293793\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293794\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293795\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293796\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293797\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293798\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293799\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293800\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293801\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293802\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293803\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293804\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293805\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293806\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293807\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293808\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293809\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293810\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293811\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293812\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293813\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293814\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293815\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293816\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293817\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293818\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293819\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293820\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293821\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293822\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293823\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293824\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293825\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293826\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293827\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293828\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293829\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293830\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293831\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293832\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293833\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293834\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293835\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293836\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293837\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293838\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293839\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293840\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293841\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293842\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293843\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293844\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293845\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293846\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293847\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293848\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293849\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293850\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293851\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293852\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293853\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293854\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293855\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293856\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293857\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293858\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293859\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293860\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293861\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293862\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293863\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293864\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293865\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293866\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293867\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293868\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293869\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293870\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293871\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293872\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293873\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293874\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293875\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293876\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293877\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293878\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293879\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293880\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293881\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293882\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293883\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293884\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293885\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293886\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293887\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293888\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293889\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293890\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293891\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293892\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293893\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293894\n",
      "Submitting model training for WS16 to HPC\n",
      "Submitted batch job 1293895\n",
      "All NM_TinyRNN jobs submitted to HPC. Check progress with 'squeue -u <username>'\n"
     ]
    }
   ],
   "source": [
    "# you can train a bunch of models with pat:\n",
    "\n",
    "reload(pat)\n",
    "#pat.run_training(overwrite=True) #OBS: this will send jobs ot the cluster !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba84abf",
   "metadata": {},
   "source": [
    "### Model performance and selection\n",
    "\n",
    "Models are trained on 80% training data, with early stopping on 10% validation data, before computing a performance score on the last 10% evaluation data.\n",
    "\n",
    "We do this with 10 random seeds and select the median performing model, for each architecture, in order to avoid overfitting on the data splits.\n",
    "\n",
    "These 'median' models are then investigated further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2653bed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you've trained a bunch of models with pat\n",
    "#Here we append the evaluation performance to the model info dataframe\n",
    "\n",
    "info_df = pat.get_train_info_df()\n",
    "evals = []\n",
    "seeds = []\n",
    "for model_info in info_df.itertuples():\n",
    "    model_dir = Path(model_info.save_path)\n",
    "    eval_json_path = model_dir / f'{model_info.model_id}_info.json'\n",
    "    if eval_json_path.exists():\n",
    "        with open(eval_json_path, 'r') as f:\n",
    "            eval_info = json.load(f)\n",
    "        evals.append(eval_info['eval_pred_loss'])\n",
    "    else:\n",
    "        evals.append(np.nan)\n",
    "info_df['eval_CE'] = evals\n",
    "#open the json and extract eval performance for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dd98635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot model performance across seeds and architectures:\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "ax = sns.stripplot(\n",
    "    data=info_df,  \n",
    "    y='eval_CE', \n",
    "    x='model_type',\n",
    "    hue='hidden_size',\n",
    "    jitter=True,\n",
    "    dodge=True, palette='Set1',\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title='hidden_size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f2ffc96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>nm_size</th>\n",
       "      <th>nm_dim</th>\n",
       "      <th>completed</th>\n",
       "      <th>eval_CE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1_unit_GRU</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.505787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_unit_LSTM</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.489911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_unit_NMRNN_1_subunits_1_low_rank</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.538541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_unit_NMRNN_2_subunits_1_low_rank</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.494762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_unit_vanilla</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.555176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_unit_GRU</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.487723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_unit_LSTM</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.487404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_unit_NMRNN_1_subunits_1_low_rank</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.491363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_unit_NMRNN_1_subunits_2_low_rank</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>0.486280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_unit_NMRNN_2_subunits_1_low_rank</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.483765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_unit_NMRNN_2_subunits_2_low_rank</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>0.485961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_unit_vanilla</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.516479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    hidden_size  nm_size  nm_dim  completed  \\\n",
       "model_id                                                                      \n",
       "1_unit_GRU                                    1        1       1       True   \n",
       "1_unit_LSTM                                   1        1       1       True   \n",
       "1_unit_NMRNN_1_subunits_1_low_rank            1        1       1       True   \n",
       "1_unit_NMRNN_2_subunits_1_low_rank            1        2       1       True   \n",
       "1_unit_vanilla                                1        1       1       True   \n",
       "2_unit_GRU                                    2        1       1       True   \n",
       "2_unit_LSTM                                   2        1       1       True   \n",
       "2_unit_NMRNN_1_subunits_1_low_rank            2        1       1       True   \n",
       "2_unit_NMRNN_1_subunits_2_low_rank            2        1       2       True   \n",
       "2_unit_NMRNN_2_subunits_1_low_rank            2        2       1       True   \n",
       "2_unit_NMRNN_2_subunits_2_low_rank            2        2       2       True   \n",
       "2_unit_vanilla                                2        1       1       True   \n",
       "\n",
       "                                     eval_CE  \n",
       "model_id                                      \n",
       "1_unit_GRU                          0.505787  \n",
       "1_unit_LSTM                         0.489911  \n",
       "1_unit_NMRNN_1_subunits_1_low_rank  0.538541  \n",
       "1_unit_NMRNN_2_subunits_1_low_rank  0.494762  \n",
       "1_unit_vanilla                      0.555176  \n",
       "2_unit_GRU                          0.487723  \n",
       "2_unit_LSTM                         0.487404  \n",
       "2_unit_NMRNN_1_subunits_1_low_rank  0.491363  \n",
       "2_unit_NMRNN_1_subunits_2_low_rank  0.486280  \n",
       "2_unit_NMRNN_2_subunits_1_low_rank  0.483765  \n",
       "2_unit_NMRNN_2_subunits_2_low_rank  0.485961  \n",
       "2_unit_vanilla                      0.516479  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_df.groupby(\"model_id\").min(\"eval_CE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77d113fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20669/1545226274.py:8: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  median_df = info_df.groupby(\"model_id\").apply(closest_to_median).reset_index(drop=True)\n",
      "/tmp/ipykernel_20669/1545226274.py:16: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  min_df = info_df.groupby(\"model_id\").apply(idx_min).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_10/GRU/1_unit_GRU_info.json → NM_TinyRNN/data/rnns/example/GRU/1_unit_GRU_info.json\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_10/GRU/1_unit_GRU_model_state.pth → NM_TinyRNN/data/rnns/example/GRU/1_unit_GRU_model_state.pth\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_10/GRU/1_unit_GRU_training_losses.htsv → NM_TinyRNN/data/rnns/example/GRU/1_unit_GRU_training_losses.htsv\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_10/GRU/1_unit_GRU_trials_data.htsv → NM_TinyRNN/data/rnns/example/GRU/1_unit_GRU_trials_data.htsv\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_8/LSTM/1_unit_LSTM_info.json → NM_TinyRNN/data/rnns/example/LSTM/1_unit_LSTM_info.json\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_8/LSTM/1_unit_LSTM_model_state.pth → NM_TinyRNN/data/rnns/example/LSTM/1_unit_LSTM_model_state.pth\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_8/LSTM/1_unit_LSTM_training_losses.htsv → NM_TinyRNN/data/rnns/example/LSTM/1_unit_LSTM_training_losses.htsv\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_8/LSTM/1_unit_LSTM_trials_data.htsv → NM_TinyRNN/data/rnns/example/LSTM/1_unit_LSTM_trials_data.htsv\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_5/NMRNN/low_rank/1_unit_NMRNN_1_subunits_1_low_rank_info.json → NM_TinyRNN/data/rnns/example/NMRNN/1_unit_NMRNN_1_subunits_1_low_rank_info.json\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_5/NMRNN/low_rank/1_unit_NMRNN_1_subunits_1_low_rank_model_state.pth → NM_TinyRNN/data/rnns/example/NMRNN/1_unit_NMRNN_1_subunits_1_low_rank_model_state.pth\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_5/NMRNN/low_rank/1_unit_NMRNN_1_subunits_1_low_rank_training_losses.htsv → NM_TinyRNN/data/rnns/example/NMRNN/1_unit_NMRNN_1_subunits_1_low_rank_training_losses.htsv\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_5/NMRNN/low_rank/1_unit_NMRNN_1_subunits_1_low_rank_trials_data.htsv → NM_TinyRNN/data/rnns/example/NMRNN/1_unit_NMRNN_1_subunits_1_low_rank_trials_data.htsv\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_7/NMRNN/low_rank/1_unit_NMRNN_2_subunits_1_low_rank_info.json → NM_TinyRNN/data/rnns/example/NMRNN/1_unit_NMRNN_2_subunits_1_low_rank_info.json\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_7/NMRNN/low_rank/1_unit_NMRNN_2_subunits_1_low_rank_model_state.pth → NM_TinyRNN/data/rnns/example/NMRNN/1_unit_NMRNN_2_subunits_1_low_rank_model_state.pth\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_7/NMRNN/low_rank/1_unit_NMRNN_2_subunits_1_low_rank_training_losses.htsv → NM_TinyRNN/data/rnns/example/NMRNN/1_unit_NMRNN_2_subunits_1_low_rank_training_losses.htsv\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_7/NMRNN/low_rank/1_unit_NMRNN_2_subunits_1_low_rank_trials_data.htsv → NM_TinyRNN/data/rnns/example/NMRNN/1_unit_NMRNN_2_subunits_1_low_rank_trials_data.htsv\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_7/vanilla/1_unit_vanilla_info.json → NM_TinyRNN/data/rnns/example/vanilla/1_unit_vanilla_info.json\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_7/vanilla/1_unit_vanilla_model_state.pth → NM_TinyRNN/data/rnns/example/vanilla/1_unit_vanilla_model_state.pth\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_7/vanilla/1_unit_vanilla_training_losses.htsv → NM_TinyRNN/data/rnns/example/vanilla/1_unit_vanilla_training_losses.htsv\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_7/vanilla/1_unit_vanilla_trials_data.htsv → NM_TinyRNN/data/rnns/example/vanilla/1_unit_vanilla_trials_data.htsv\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_6/GRU/2_unit_GRU_info.json → NM_TinyRNN/data/rnns/example/GRU/2_unit_GRU_info.json\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_6/GRU/2_unit_GRU_model_state.pth → NM_TinyRNN/data/rnns/example/GRU/2_unit_GRU_model_state.pth\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_6/GRU/2_unit_GRU_training_losses.htsv → NM_TinyRNN/data/rnns/example/GRU/2_unit_GRU_training_losses.htsv\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_6/GRU/2_unit_GRU_trials_data.htsv → NM_TinyRNN/data/rnns/example/GRU/2_unit_GRU_trials_data.htsv\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_6/LSTM/2_unit_LSTM_info.json → NM_TinyRNN/data/rnns/example/LSTM/2_unit_LSTM_info.json\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_6/LSTM/2_unit_LSTM_model_state.pth → NM_TinyRNN/data/rnns/example/LSTM/2_unit_LSTM_model_state.pth\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_6/LSTM/2_unit_LSTM_training_losses.htsv → NM_TinyRNN/data/rnns/example/LSTM/2_unit_LSTM_training_losses.htsv\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_6/LSTM/2_unit_LSTM_trials_data.htsv → NM_TinyRNN/data/rnns/example/LSTM/2_unit_LSTM_trials_data.htsv\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_6/NMRNN/low_rank/2_unit_NMRNN_1_subunits_1_low_rank_info.json → NM_TinyRNN/data/rnns/example/NMRNN/2_unit_NMRNN_1_subunits_1_low_rank_info.json\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_6/NMRNN/low_rank/2_unit_NMRNN_1_subunits_1_low_rank_model_state.pth → NM_TinyRNN/data/rnns/example/NMRNN/2_unit_NMRNN_1_subunits_1_low_rank_model_state.pth\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_6/NMRNN/low_rank/2_unit_NMRNN_1_subunits_1_low_rank_training_losses.htsv → NM_TinyRNN/data/rnns/example/NMRNN/2_unit_NMRNN_1_subunits_1_low_rank_training_losses.htsv\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_6/NMRNN/low_rank/2_unit_NMRNN_1_subunits_1_low_rank_trials_data.htsv → NM_TinyRNN/data/rnns/example/NMRNN/2_unit_NMRNN_1_subunits_1_low_rank_trials_data.htsv\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_6/NMRNN/low_rank/2_unit_NMRNN_1_subunits_2_low_rank_info.json → NM_TinyRNN/data/rnns/example/NMRNN/2_unit_NMRNN_1_subunits_2_low_rank_info.json\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_6/NMRNN/low_rank/2_unit_NMRNN_1_subunits_2_low_rank_model_state.pth → NM_TinyRNN/data/rnns/example/NMRNN/2_unit_NMRNN_1_subunits_2_low_rank_model_state.pth\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_6/NMRNN/low_rank/2_unit_NMRNN_1_subunits_2_low_rank_training_losses.htsv → NM_TinyRNN/data/rnns/example/NMRNN/2_unit_NMRNN_1_subunits_2_low_rank_training_losses.htsv\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_6/NMRNN/low_rank/2_unit_NMRNN_1_subunits_2_low_rank_trials_data.htsv → NM_TinyRNN/data/rnns/example/NMRNN/2_unit_NMRNN_1_subunits_2_low_rank_trials_data.htsv\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_9/NMRNN/low_rank/2_unit_NMRNN_2_subunits_1_low_rank_info.json → NM_TinyRNN/data/rnns/example/NMRNN/2_unit_NMRNN_2_subunits_1_low_rank_info.json\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_9/NMRNN/low_rank/2_unit_NMRNN_2_subunits_1_low_rank_model_state.pth → NM_TinyRNN/data/rnns/example/NMRNN/2_unit_NMRNN_2_subunits_1_low_rank_model_state.pth\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_9/NMRNN/low_rank/2_unit_NMRNN_2_subunits_1_low_rank_training_losses.htsv → NM_TinyRNN/data/rnns/example/NMRNN/2_unit_NMRNN_2_subunits_1_low_rank_training_losses.htsv\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_9/NMRNN/low_rank/2_unit_NMRNN_2_subunits_1_low_rank_trials_data.htsv → NM_TinyRNN/data/rnns/example/NMRNN/2_unit_NMRNN_2_subunits_1_low_rank_trials_data.htsv\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_10/NMRNN/low_rank/2_unit_NMRNN_2_subunits_2_low_rank_info.json → NM_TinyRNN/data/rnns/example/NMRNN/2_unit_NMRNN_2_subunits_2_low_rank_info.json\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_10/NMRNN/low_rank/2_unit_NMRNN_2_subunits_2_low_rank_model_state.pth → NM_TinyRNN/data/rnns/example/NMRNN/2_unit_NMRNN_2_subunits_2_low_rank_model_state.pth\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_10/NMRNN/low_rank/2_unit_NMRNN_2_subunits_2_low_rank_training_losses.htsv → NM_TinyRNN/data/rnns/example/NMRNN/2_unit_NMRNN_2_subunits_2_low_rank_training_losses.htsv\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_10/NMRNN/low_rank/2_unit_NMRNN_2_subunits_2_low_rank_trials_data.htsv → NM_TinyRNN/data/rnns/example/NMRNN/2_unit_NMRNN_2_subunits_2_low_rank_trials_data.htsv\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_3/vanilla/2_unit_vanilla_info.json → NM_TinyRNN/data/rnns/example/vanilla/2_unit_vanilla_info.json\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_3/vanilla/2_unit_vanilla_model_state.pth → NM_TinyRNN/data/rnns/example/vanilla/2_unit_vanilla_model_state.pth\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_3/vanilla/2_unit_vanilla_training_losses.htsv → NM_TinyRNN/data/rnns/example/vanilla/2_unit_vanilla_training_losses.htsv\n",
      "Copied NM_TinyRNN/data/rnns/WS16/random_seed_3/vanilla/2_unit_vanilla_trials_data.htsv → NM_TinyRNN/data/rnns/example/vanilla/2_unit_vanilla_trials_data.htsv\n"
     ]
    }
   ],
   "source": [
    "## Code to get 'median_df' and copy over the median-performing models for testing and comparisons\n",
    "\n",
    "def closest_to_median(subdf):\n",
    "    med = subdf[\"eval_CE\"].median()\n",
    "    idx = (subdf[\"eval_CE\"] - med).abs().idxmin()\n",
    "    return subdf.loc[idx, :]\n",
    "\n",
    "median_df = info_df.groupby(\"model_id\").apply(closest_to_median).reset_index(drop=True)\n",
    "median_df\n",
    "\n",
    "def idx_min(subdf):\n",
    "    min = subdf[\"eval_CE\"].min()\n",
    "    idx = (subdf[\"eval_CE\"] - min).abs().idxmin()\n",
    "    return subdf.loc[idx, :]\n",
    "\n",
    "min_df = info_df.groupby(\"model_id\").apply(idx_min).reset_index(drop=True)\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def copy_median_models(df, example_path):\n",
    "    \"\"\"\n",
    "    Copy median performing model files into EXAMPLE_PATH/<model_type>/.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must contain columns ['save_path', 'model_type', 'model_ID'].\n",
    "        save_path should be a pathlib.Path pointing to the model directory.\n",
    "    example_path : str or Path\n",
    "        Parent directory where copies will be stored.\n",
    "    \"\"\"\n",
    "    example_path = Path(example_path)\n",
    "    example_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # files we want to copy for each model\n",
    "    file_suffixes = [\n",
    "        \"_info.json\",\n",
    "        \"_model_state.pth\",\n",
    "        \"_training_losses.htsv\",\n",
    "        \"_trials_data.htsv\",\n",
    "    ]\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        save_path = Path(row[\"save_path\"])\n",
    "        model_type = row[\"model_type\"]\n",
    "        model_id = row[\"model_id\"]\n",
    "\n",
    "        # make sure destination subfolder exists\n",
    "        dest_dir = example_path / model_type\n",
    "        dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for suffix in file_suffixes:\n",
    "            src = save_path / f\"{model_id}{suffix}\"\n",
    "            dst = dest_dir / f\"{model_id}{suffix}\"\n",
    "            if src.exists():\n",
    "                shutil.copy(src, dst)\n",
    "                print(f\"Copied {src} → {dst}\")\n",
    "            else:\n",
    "                print(f\"Warning: {src} not found!\")\n",
    "\n",
    "# Example usage:\n",
    "copy_median_models(min_df, Path('./NM_TinyRNN/data/rnns/example'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
