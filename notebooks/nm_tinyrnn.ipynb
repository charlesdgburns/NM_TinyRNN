{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQRJyEAVXg0lbOCNqNo89R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charlesdgburns/NM_TinyRNN/blob/main/notebooks/nm_tinyrnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Let's use this as a learning notebook about gated recurrent units.\n",
        "\n",
        "We will fit these to sequential behavioural decision making later.\n",
        "\n",
        "We are taking inspiration from the following blogpost:\n",
        "https://medium.com/data-science/building-a-lstm-by-hand-on-pytorch-59c02a4ec091\n",
        "\n",
        "**The information flow in a gated recurrent unit**\n",
        "\n",
        "A gated recurrent unit has information flowing from the inputs $^{(1)}$ $x_{t-1}$  and the past hidden $h_{t-1}$, which is gated via 'reset' and 'update' gates $r_t$ and $z_t$ before giving the final output $h_t$. The update gate decides whether or not to overwrite a long-term memory with inputs, while the\n",
        "\n",
        "The gated recurrent unit allows the recurrent unit to persist its state and ignore its inputs.\n",
        "\n",
        "\n",
        "$$ r_t = \\sigma(W_{ir}x_{t-1}+b_{ir}+W_{hr}h_{t-1}+b_{hr}) \\qquad \\text{(reset)}$$\n",
        "\n",
        "$$ z_t = \\sigma(W_{iz}x_{t-1}+b_{iz}+W_{hz}h_{t-1}+b_{hz}) \\qquad\\text{(update)}$$\n",
        "\n",
        "$$ n_t = \\tanh(W_{in}x_{t-1}+b_{in}+r_{t}\\odot (W_{hn}h_{t-1}+b_{hn})) \\qquad\\text{(new)} $$\n",
        "\n",
        "$$ h_t = (1-z_t)\\odot n_t +z_t \\odot h_{t-1} \\qquad\\text{(gated reccurent output)}$$\n",
        "\n",
        "*footnotes*\n",
        "(1): inputs at time $t$ is recent experience\n",
        "\n",
        "Note that GRU's are markovian - their current state ($h_t$) can be determined entirely from its previous state ($h_{t-1}$) and inputs ($x_{t-1}$)."
      ],
      "metadata": {
        "id": "U5W-ZRWU03M9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ky3ugmGf0fBW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "class ManualGRU(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size, batch_first = False):\n",
        "    self.sigmoid = torch.nn.Sigmoid()\n",
        "    self.tanh = torch.nn.Tanh()\n",
        "\n",
        "    self.W_from_in = nn.Parameter(torch.Tensor(input_size, hidden_size*3))\n",
        "    self.W_from_h = nn.Parameter(torch.Tensor(hidden_size, hidden_size*3))\n",
        "    self.bias = nn.Parameter(torch.Tensor(hidden_size*6))\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "    for weight in self.parameters():\n",
        "        weight.data.uniform_(-stdv, stdv)\n",
        "  def forward(self, inputs, init_states = None):\n",
        "    ''' inputs are a tensor of shape (batch_size, sequence_size, input_size)\n",
        "        outputs are tensor of shape (batch_size, sequence_size, hidden_size)'''\n",
        "\n",
        "    batch_size, sequence_size, _ = inputs.size\n",
        "    hidden_sequence = []\n",
        "    if init_states is None:\n",
        "      h_past = torch.zeros(batch_size, self.hidden_size).to(inputs.device)\n",
        "    else:\n",
        "      h_past = init_states\n",
        "\n",
        "    for t in range(sequence_size):\n",
        "      x_past = inputs[:,t,:] #(n_batch,input_size)\n",
        "      #for computational efficiency we do two matrix multiplications and then do indexing further down:\n",
        "      from_input = x_past@self.W_from_in + self.bias[:3]  #(n_batch,n_hidden)\n",
        "      from_hidden = h_past@self.W_from_h + self.bias[3:]  #(n_batch,n_hidden)\n",
        "\n",
        "      r_t =self.sigmoid(from_input[0]+from_hidden[0]) #(n_batch,n_hidden), ranging from 0 to 1\n",
        "      z_t = self.sigmoid(from_input[1]+from_hidden[1]) #(n_batch,n_hidden), ranging from 0 to 1\n",
        "      n_t = self.tanh(from_input[2]+r_t*(from_hidden[2])) #(n_batch,n_hidden)\n",
        "      h_past = (1-z_t)*n_t + z_t*h_past #(n_batch,hidden_size) #NOTE h_past is tehnically h_t now, but in the next for-loop it will be h_past. ;)\n",
        "      hidden_sequence.append(h_past.unsqueeze(0)) #appending (1,n_batch,n_hidden) to a big list.\n",
        "    hidden_sequence = torch.cat(hidden_sequence, dim=0) #(n_sequence, n_batch, n_hidden) gather all inputs along the first dimenstion\n",
        "    hidden_sequence = hidden_sequence.transpose(0, 1).contiguous() #reshape to batch first (n_batch,n_seq,n_hidden)\n",
        "    return hidden_sequence, h_past #this is standard in Pytorch, to output sequence of hidden states alongside most recent hidden state.\n",
        "\n",
        "## for reference, look at a hand-crafted LSTM from the following tutorial #\n",
        "\n",
        "\n",
        "\n",
        "class ManualSTM(nn.Module):\n",
        "    def __init__(self, input_sz, hidden_sz, device):\n",
        "        super().__init__()\n",
        "        self.device=device\n",
        "        self.input_sz = input_sz\n",
        "        self.hidden_size = hidden_sz\n",
        "        self.W = nn.Parameter(torch.Tensor(input_sz, hidden_sz * 4))\n",
        "        self.U = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz * 4))\n",
        "        self.bias = nn.Parameter(torch.Tensor(hidden_sz * 6))\n",
        "        self.init_weights()\n",
        "\n",
        "\n",
        "    def init_weights(self):\n",
        "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv).to(self.device)\n",
        "\n",
        "    def forward(self, x,\n",
        "                init_states=None):\n",
        "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
        "        bs, seq_sz, _ = x.size()\n",
        "        hidden_seq = []\n",
        "        if init_states is None:\n",
        "            h_t, c_t = (torch.zeros(bs, self.hidden_size).to(x.device),\n",
        "                        torch.zeros(bs, self.hidden_size).to(x.device))\n",
        "        else:\n",
        "            h_t, c_t = init_states\n",
        "\n",
        "        HS = self.hidden_size\n",
        "        for t in range(seq_sz):\n",
        "            x_t = x[:, t, :]\n",
        "            # batch the computations into a single matrix multiplication\n",
        "            gates = x_t @ self.W + h_t @ self.U + self.bias\n",
        "            i_t, f_t, g_t, o_t = (\n",
        "                torch.sigmoid(gates[:, :HS]), # input\n",
        "                torch.sigmoid(gates[:, HS:HS*2]), # forget\n",
        "                torch.tanh(gates[:, HS*2:HS*3]),\n",
        "                torch.sigmoid(gates[:, HS*3:]), # output\n",
        "            )\n",
        "            c_t = f_t * c_t + i_t * g_t\n",
        "            h_t = o_t * torch.tanh(c_t)\n",
        "            hidden_seq.append(h_t.unsqueeze(0))\n",
        "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
        "        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
        "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
        "        return hidden_seq, (h_t, c_t)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9jXAF4YRJuwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NM-RNN\n",
        "\n",
        "[Costacura et al. (2024)](https://openreview.net/pdf?id=HbIBqn3grD) introduced NM-RNNs to bridge a gap between more standard RNNs today and biophysical models.\n",
        "\n",
        "Here we rewrite the equations (1) to (4) in Costacura et al. (2024) more similarly to the standard RNN notation above.\n",
        "\n",
        "We have inputs $x_{t-1}$ and our past hidden state $h_{t-1}$ which we want to integrate to get a new hidden state $h_t$. However, we want to selectively change (by a gain) the weights of our recurrent network depending on a neuromodulation signal $s(z(t))$.\n",
        "\n",
        "We therefore have a coupled network system, starting from a subnetwork state $z(t)$. In discretised terms:\n",
        "$$ \\tau_{z}  z_t = W_{zz} \\phi(z_t)+W_{iz} x_{t-1} \\qquad (1) $$\n",
        "\n",
        "$$ \\tau_{x} h_{t-1} = W_x(z_t)\\cdot\\phi(h_{t-1})+W_{ih} \\qquad (2)$$\n",
        "\n",
        "$$s(z(t)) = \\sigma(W_s z_t+ b_s) \\qquad W_x(z_t)=\\sum_{k=1}^K s_k(z_t)\\mathcal{l}_k r_k^T \\qquad (4)$$\n",
        "\n",
        "Note that instead of a low-rank recurrent weight component, we want a tiny RNN, so we could modulating the weights associated with a given unit. Now the 'dynamic modes' are not the low ranks of a large network, but the activity of single units."
      ],
      "metadata": {
        "id": "TK6-Am2hOMf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ManualNMRNN(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size, batch_first = False):\n",
        "    self.sigmoid = torch.nn.Sigmoid()\n",
        "    self.tanh = torch.nn.Tanh()\n",
        "\n",
        "    self.W_from_in = nn.Parameter(torch.Tensor(input_size, hidden_size*2))\n",
        "    self.W_from_h = nn.Parameter(torch.Tensor(hidden_size, hidden_size*3))\n",
        "    self.bias = nn.Parameter(torch.Tensor(hidden_size*6))\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "    for weight in self.parameters():\n",
        "        weight.data.uniform_(-stdv, stdv)\n",
        "  def forward(self, inputs, init_states = None):\n",
        "    ''' inputs are a tensor of shape (batch_size, sequence_size, input_size)\n",
        "        outputs are tensor of shape (batch_size, sequence_size, hidden_size)'''\n",
        "\n",
        "    batch_size, sequence_size, _ = inputs.size\n",
        "    hidden_sequence = []\n",
        "    if init_states is None:\n",
        "      h_past = torch.zeros(batch_size, self.hidden_size).to(inputs.device)\n",
        "    else:\n",
        "      h_past = init_states\n",
        "\n",
        "    for t in range(sequence_size):\n",
        "      x_past = inputs[:,t,:] #(n_batch,input_size)\n",
        "      #for computational efficiency we do two matrix multiplications and then do indexing further down:\n",
        "      from_input = x_past@self.W_from_in + self.bias[:3]  #(n_batch,n_hidden)\n",
        "      from_hidden = h_past@self.W_from_h + self.bias[3:]  #(n_batch,n_hidden)\n",
        "\n",
        "      r_t =self.sigmoid(from_input[0]+from_hidden[0]) #(n_batch,n_hidden), ranging from 0 to 1\n",
        "      z_t = self.sigmoid(from_input[1]+from_hidden[1]) #(n_batch,n_hidden), ranging from 0 to 1\n",
        "      n_t = self.tanh(from_input[2]+r_t*(from_hidden[2])) #(n_batch,n_hidden)\n",
        "      h_past = (1-z_t)*n_t + z_t*h_past #(n_batch,hidden_size) #NOTE h_past is tehnically h_t now, but in the next for-loop it will be h_past. ;)\n",
        "      hidden_sequence.append(h_past.unsqueeze(0)) #appending (1,n_batch,n_hidden) to a big list.\n",
        "    hidden_sequence = torch.cat(hidden_sequence, dim=0) #(n_sequence, n_batch, n_hidden) gather all inputs along the first dimenstion\n",
        "    hidden_sequence = hidden_sequence.transpose(0, 1).contiguous() #reshape to batch first (n_batch,n_seq,n_hidden)\n",
        "    return hidden_sequence, h_past #this is standard in Pytorch, to output sequence of hidden states alongside most recent hidden state.\n"
      ],
      "metadata": {
        "id": "xk47vkMkWps9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}