{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charlesdgburns/NM_TinyRNN/blob/main/notebooks/understanding_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5W-ZRWU03M9"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Let's use this as a learning notebook about different RNN architectures.\n",
        "We want to start from scratch and get an idea of how different architectures work before fitting to some data.\n",
        "\n",
        "\n",
        "We the fit these to sequential behavioural decision making later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaPBpmAS_mMk",
        "outputId": "c985a49a-7b4d-427a-fb4c-72b59c21f75e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NM_TinyRNN'...\n",
            "remote: Enumerating objects: 191, done.\u001b[K\n",
            "remote: Counting objects: 100% (191/191), done.\u001b[K\n",
            "remote: Compressing objects: 100% (128/128), done.\u001b[K\n",
            "remote: Total 191 (delta 74), reused 112 (delta 33), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (191/191), 788.23 KiB | 7.30 MiB/s, done.\n",
            "Resolving deltas: 100% (74/74), done.\n"
          ]
        }
      ],
      "source": [
        "## setup on google colab:\n",
        "\n",
        "!git clone https://github.com/charlesdgburns/NM_TinyRNN.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNzAHyfqAuHq"
      },
      "outputs": [],
      "source": [
        "# setup\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBVkup08mkxr"
      },
      "source": [
        "# Recurrent Neural Network (RNN)\n",
        "\n",
        "The simplest of all recurrent architectures, which we think of as updating a hidden state $h_t$ using sensory inputs $x_{t-1}$ and recurrent inputs from the last hidden state $h_{t-1}$. Formally, this is described as a single discretised update:\n",
        "\n",
        "$$ h_t = \\tanh(W_{ih} x_{t-1} + b_{i} + W_{hh} h_{t-1} +b_{hh}) $$\n",
        "\n",
        "where $W_{ih}$ are the weights from the input to the hidden state, $W_{hh}$ are the recurrent weights from the hidden state to itself, and we consider additional biases $b_{i}$ and $b_{hh}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psieepNUAqte"
      },
      "outputs": [],
      "source": [
        "# let's do this later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-UbowrBmpK7"
      },
      "source": [
        "# Gated Recurrent Unit (GRU)\n",
        "\n",
        "We are taking inspiration from the following blogpost:\n",
        "https://medium.com/data-science/building-a-lstm-by-hand-on-pytorch-59c02a4ec091\n",
        "\n",
        "**The information flow in a gated recurrent unit**\n",
        "\n",
        "A gated recurrent unit has information flowing from the inputs $^{(1)}$ $x_{t-1}$  and the past hidden $h_{t-1}$, which is gated via 'reset' and 'update' gates $r_t$ and $z_t$ before giving the final output $h_t$. The update gate decides whether or not to overwrite a long-term memory with inputs, while the\n",
        "\n",
        "The gated recurrent unit allows the recurrent unit to persist its state and ignore its inputs.\n",
        "\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "r_t &= \\sigma(W_{ir}x_{t-1}+W_{hr}h_{t-1}+b_{r})  &&\\text{(reset gate)} \\\\\n",
        "z_t &= \\sigma(W_{iz}x_{t-1}+W_{hz}h_{t-1}+b_{z})  &&\\text{(update gate)} \\\\\n",
        "n_t &= \\tanh(W_{in}x_{t-1}+b_{in}+r_{t}\\cdot (W_{hn}h_{t-1}+b_{hn})) &&\\text{(new state)}\n",
        "\\\\\n",
        "h_t &= (1-z_t)\\cdot h_{t-1} +z_t \\cdot n_t &&\\text{(hidden state)}\n",
        "\\end{align}\n",
        "\n",
        "*footnotes*\n",
        "\n",
        "Note that GRU's are markovian - their current state ($h_t$) can be determined entirely from its previous state ($h_{t-1}$) and inputs ($x_{t-1}$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ky3ugmGf0fBW",
        "outputId": "51dfbaf3-65a7-4acb-a7c5-e250b9155b5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8, 150, 2])"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ManualGRU(nn.Module):\n",
        "  '''Manual GRU coded to return gate activations dictionary\n",
        "  Returns: (hidden_sequence, h_past) if return_gate_activations=False,\n",
        "           (hidden_sequence, gate_activations) if -||- = True'''\n",
        "  def __init__(self,input_size,hidden_size):\n",
        "    super().__init__() #init nn.Module\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.sigmoid = torch.nn.Sigmoid()\n",
        "    self.tanh = torch.nn.Tanh()\n",
        "\n",
        "    self.W_from_in = nn.Parameter(torch.Tensor(input_size, hidden_size*3))\n",
        "    self.W_from_h = nn.Parameter(torch.Tensor(hidden_size, hidden_size*3))\n",
        "    self.bias = nn.Parameter(torch.Tensor(hidden_size*6))\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "    for weight in self.parameters():\n",
        "        weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "  def forward(self, inputs, init_states = None,\n",
        "              return_gate_activations=False):\n",
        "    ''' inputs are a tensor of shape (batch_size, sequence_size, input_size)\n",
        "        outputs are tensor of shape (batch_size, sequence_size, hidden_size)'''\n",
        "\n",
        "    batch_size, sequence_size, _ = inputs.shape\n",
        "    hidden_sequence = []\n",
        "    if return_gate_activations:\n",
        "      gate_activations = {'reset':[],'update':[]}\n",
        "    if init_states is None:\n",
        "      h_past = torch.zeros(batch_size, self.hidden_size).to(inputs.device) #(n_hidden,batch_size)\n",
        "    else:\n",
        "      h_past = init_states\n",
        "\n",
        "    for t in range(sequence_size):\n",
        "      x_past = inputs[:,t,:] #(n_batch,input_size)\n",
        "      #for computational efficiency we do two matrix multiplications and then do indexing further down:\n",
        "      from_input = x_past@self.W_from_in + self.bias[:3*self.hidden_size]\n",
        "      from_input = from_input.view(batch_size,3, self.hidden_size) #(n_batch,3,n_hidden)\n",
        "      from_hidden = h_past@self.W_from_h + self.bias[3*self.hidden_size:]\n",
        "      from_hidden = from_hidden.view(batch_size, 3, self.hidden_size) #(n_batch,3, n_hidden)\n",
        "      r_t = self.sigmoid(from_input[:,0]+from_hidden[:,0]) #(n_batch,n_hidden), ranging from 0 to 1\n",
        "      z_t = self.sigmoid(from_input[:,1]+from_hidden[:,1]) #(n_batch,n_hidden), ranging from 0 to 1; must have n_hidden because it is multiplied with hidden_state later.\n",
        "      if return_gate_activations:\n",
        "        gate_activations['reset'].append(r_t.unsqueeze(0))\n",
        "        gate_activations['update'].append(z_t.unsqueeze(0))\n",
        "      n_t = self.tanh(from_input[:,2]+r_t*(from_hidden[:,2])).view(batch_size, self.hidden_size) #(n_batch,n_hidden)\n",
        "      h_past = (1-z_t)*n_t + z_t*h_past #(n_batch,hidden_size) #NOTE h_past is tehnically h_t now, but in the next for-loop it will be h_past. ;)\n",
        "      hidden_sequence.append(h_past.unsqueeze(0)) #appending (1,n_batch,n_hidden) to a big list.\n",
        "\n",
        "    hidden_sequence = torch.cat(hidden_sequence, dim=0) #(n_sequence, n_batch, n_hidden) gather all inputs along the first dimenstion\n",
        "    hidden_sequence = hidden_sequence.transpose(0, 1).contiguous() #reshape to batch first (n_batch,n_seq,n_hidden)\n",
        "\n",
        "    if return_gate_activations:\n",
        "        for gate_label, activations in gate_activations.items():\n",
        "            gate_activations[gate_label] = torch.cat(activations, dim=0).transpose(0,1).contiguous() #(n_batch,n_seq,n_hidden)\n",
        "        return hidden_sequence, gate_activations\n",
        "    else:\n",
        "      return hidden_sequence, h_past #this is standard in Pytorch, to output sequence of hidden states alongside most recent hidden state.\n",
        "\n",
        "test_inputs = torch.ones((8,150,3))\n",
        "test = ManualGRU(3, 2)\n",
        "hidden_sequence, h_past = test(test_inputs)\n",
        "hidden_sequence, gate_activations = test(test_inputs,return_gate_activations=True)\n",
        "gate_activations['reset'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVOFZkNDm9zE"
      },
      "source": [
        "# Long-Short-Term-Memory\n",
        "\n",
        "Here we implement a standard LSTM layer by layer.\n",
        "\n",
        "- Following https://medium.com/data-science/building-a-lstm-by-hand-on-pytorch-59c02a4ec091\n",
        "\n",
        "### Maths:\n",
        "\n",
        "\\begin{align}\n",
        " f_t &=\\sigma(W_{if} x_t+W_{hf} h_{t-1}+b_f) &&\\text{(forget gate)}\n",
        " \\\\\n",
        " i_t &= \\sigma(W_{ii} x_t+W_{ih} h_{t-1}+b_i)  &&\\text{(input gate)}\n",
        " \\\\\n",
        " o_t &=  \\sigma(W_{io}  x_t+W_{oh} h_{t-1}+b_o) &&\\text{(output gate)}\n",
        " \\\\\n",
        " \\tilde{C}_t & = \\tanh(W_{ic}x_{t-1}+W_{hc}h_{t-1}+b_c) &&\\text{(partial candidate)}\n",
        " \\\\\n",
        " C_t&=f_t\\odot C_{t-1} + \\tilde{C}_t &&\\text{(candidate / LTM)}\\\\\n",
        " h_t & = o_t \\odot \\tanh(C_t) &&\\text{(hidden / STM)}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\n",
        "## Classic blog post explanation goes like:\n",
        "### 1. Forge gate\n",
        "\n",
        "The forget gate is where input, $x_t$, is combined with long term memory (termed the 'candidate'), $C_{t-1}$, and short term memory (the hidden state), $h_{t-1}$ . That is:\n",
        "\n",
        "$$ f_t =\\sigma(U_f x_t+V_f h_{t-1}+b_f),\\qquad C'_t=f_t\\cdot C_{t-1}$$\n",
        "\n",
        "where $\\sigma\\mapsto (0,1)$ is a sigmoidal activation function. The term $C'_t$ represents how much of the long term memory is retrieved.\n",
        "\n",
        "### 2. Input gate\n",
        "\n",
        "Where input informaion and hidden states are combined along with partial candidate $C'_t$ to form a new candidate:\n",
        "\n",
        "$$ i_t = \\sigma(U_ix_t+V_ih_{t-1}+b_i) \\\\\n",
        "C_t^+=\\tanh(U_c x_t+V_c h_{t-1} +b_c) \\\\\n",
        "C_t= C'_t+i_t\\cdot C_t^+$$\n",
        "\n",
        "Note here that $\\tanh\\mapsto (-1,1)$ affects how much information will be introduced to the memory.\n",
        "\n",
        "### 3. Output gate and hidden state of the cell\n",
        "\n",
        "Lastly we combine the l short term memory ($h_{t-1}$) and inputs ($x_t$) at the output gate, which is finally combined with long term memory (C_t) to get out final output ($h_t$).\n",
        "\n",
        "$$o_t=\\sigma(U_o  x_t+V_o h_{t-1}+b_o) \\\\ h_t = o_t\\cdot \\tanh(C_t) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcmaEa-A-gFc"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class ManualLSTM(nn.Module):\n",
        "    def __init__(self, input_sz, hidden_sz, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.device=device\n",
        "        self.input_sz = input_sz\n",
        "        self.hidden_size = hidden_sz\n",
        "        self.W = nn.Parameter(torch.Tensor(input_sz, hidden_sz * 4))\n",
        "        self.U = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz * 4))\n",
        "        self.bias = nn.Parameter(torch.Tensor(hidden_sz * 4))\n",
        "        self.init_weights()\n",
        "\n",
        "\n",
        "    def init_weights(self):\n",
        "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv).to(self.device)\n",
        "\n",
        "    def forward(self, x, init_states=None,\n",
        "                return_gate_activations = False):\n",
        "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
        "        bs, seq_sz, _ = x.size() #let's get our shapes right\n",
        "        HS = self.hidden_size\n",
        "        if init_states is None:\n",
        "            h_t, c_t = (torch.zeros(bs, self.hidden_size).to(x.device),\n",
        "                        torch.zeros(bs, self.hidden_size).to(x.device))\n",
        "        else:\n",
        "            h_t, c_t = init_states\n",
        "\n",
        "        hidden_sequence = []#initialise lists for unit activations\n",
        "        if return_gate_activations:\n",
        "            gate_activations = {'input':[],'forget':[],'output':[], 'candidate':[]}\n",
        "        for t in range(seq_sz):\n",
        "            x_t = x[:, t, :]\n",
        "            # batch the computations into a single matrix multiplication\n",
        "            gates = x_t @ self.W + h_t @ self.U + self.bias\n",
        "            i_t, f_t, g_t, o_t = (\n",
        "                torch.sigmoid(gates[:, :HS]), # input\n",
        "                torch.sigmoid(gates[:, HS:HS*2]), # forget\n",
        "                torch.tanh(gates[:, HS*2:HS*3]), # this integrates inputs and past hidden (so over short-term memory)\n",
        "                torch.sigmoid(gates[:, HS*3:]), # output\n",
        "            )\n",
        "            c_t = f_t * c_t + i_t * g_t #here we integrate inputs/STM with long-term memory// never update candidate if forget = 1 and input = 0.\n",
        "            h_t = o_t * torch.tanh(c_t)\n",
        "            hidden_sequence.append(h_t.unsqueeze(0))\n",
        "            if return_gate_activations:\n",
        "                for gate_label, activation in {'input':i_t,'forget':f_t,'output':o_t,'candidate':c_t}.items():\n",
        "                    gate_activations[gate_label].append(activation.unsqueeze(0))\n",
        "        hidden_sequence = torch.cat(hidden_sequence, dim=0) #(sequence, batch, n_hidden)\n",
        "        hidden_sequence = hidden_sequence.transpose(0, 1).contiguous() #(batch, sequence, feature)\n",
        "        if return_gate_activations:\n",
        "            for gate_label, activations in gate_activations.items():\n",
        "                gate_activations[gate_label] = torch.cat(activations, dim=0).transpose(0,1).contiguous() #(n_batch,n_seq,n_hidden)\n",
        "            return hidden_sequence, gate_activations\n",
        "        else:\n",
        "            return hidden_sequence, (h_t, c_t)\n",
        "\n",
        "test_inputs = torch.ones((8,10,3))\n",
        "testLSTM = ManualLSTM(3,2)\n",
        "hidden_seq, hidden_tuple = testLSTM(test_inputs)\n",
        "hidden_seq, hidden_tuple = testLSTM(test_inputs, init_states = hidden_tuple)\n",
        "hidden_seq, gate_activations = testLSTM(test_inputs,\n",
        "                                        return_gate_activations = True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK6-Am2hOMf8"
      },
      "source": [
        "# NM-RNN\n",
        "\n",
        "[Costacura et al. (2024)](https://openreview.net/pdf?id=HbIBqn3grD) introduced NM-RNNs to bridge a gap between more standard RNNs today and biophysical models.\n",
        "\n",
        "Here we rewrite the equations (1) to (4) in Costacura et al. (2024) more similarly to the standard RNN notation above.\n",
        "\n",
        "We have inputs $x_{t-1}$ and our past hidden state $h_{t-1}$ which we want to integrate to get a new hidden state $h_t$. However, we want to selectively change (by a gain) the weights of our recurrent network depending on a neuromodulation signal $s(z(t))$.\n",
        "\n",
        "We therefore have a coupled network system, starting from a subnetwork state $z(t)$. Consider the following discretisation (see footnote below for comparison to costacurta et al's paper):\n",
        "\n",
        "$$ z_t = \\phi ( W_{zz}(z_{t-1})+W_{iz} x_{t-1} ) \\qquad (1; \\text{nm subnetwork}) $$\n",
        "\n",
        "$$ h_{t-1} =\\phi( W_{hh}(z_t)h_{t-1}+W_{ih} x_{t_i} )\\qquad (2; \\text{recurrent output})$$\n",
        "\n",
        "$$s(z(t)) = \\sigma(W_{zk} z_t+ b_k) \\qquad (3; \\text{modulation gating}) $$\n",
        "$$W_x(z_t)=\\sum_{k=1}^K s_k(z_t)\\mathcal{l}_k r_k^T \\qquad (4; \\text{recurrent weight modulation})$$\n",
        "\n",
        "Note that instead of a low-rank recurrent weight component, we want a tiny RNN, so we could modulating the weights associated with a given unit. Now the 'dynamic modes' are not the low ranks of a large network, but the activity of single units.\n",
        "\n",
        "\n",
        "Importantly, in 1D we have at most a 1-rank decomposition as the recurrent weights, $W_{hh} \\in \\mathbb{R}$, are a scalar.\n",
        "\n",
        "In 2D, if our two units are $A$ and $B$, we now have $W_{hh} \\in \\mathbb{R}^{2\\times 2}$, a 2x2 matrix.\n",
        "This matrix can now be decomposed into a 2-rank decomposition, each component representing an orthogonal direction in the recurrent dynamics of the 2D hidden state.\n",
        "\n",
        "$$ W_{hh} =\n",
        "\\begin{bmatrix}\n",
        "W_{AA} & W_{AB} \\\\\n",
        "W_{BA} & W_{BB} \\\\\n",
        "\\end{bmatrix} = \\mathbb{l}_1 r_1^{T} + \\mathbb{l}_2 r_2^{T} + \\mathcal{e}\n",
        "$$\n",
        "\n",
        "in practice we use an SVD decomposition of $W_{hh}$.\n",
        "\n",
        "However, since we have a tiny RNN, we can imagine other decompositions of $W_{hh}$, such as column-wise or row-wise.\n",
        "$$ W_{hh} =  \n",
        "\\begin{bmatrix}\n",
        "W_{AA} & W_{AB} \\\\\n",
        "W_{BA} & W_{BB} \\\\\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "W_{AA} & 0\\\\\n",
        "W_{BA} & 0\\\\\n",
        "\\end{bmatrix} +\n",
        "\\begin{bmatrix}\n",
        "0 & W_{AB}\\\\\n",
        "0 & W_{BB} \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "W_{AA} & W_{AB}\\\\\n",
        "0 & 0\\\\\n",
        "\\end{bmatrix} +\n",
        "\\begin{bmatrix}\n",
        "0 & 0\\\\\n",
        "W_{BA} & W_{BB} \\\\\n",
        "\\end{bmatrix} $$\n",
        "\n",
        "Now we can interpret scaling th column-wise decomposition as pre-synaptic modulation for each unit (first matrix controls recurrent inputs pre-synaptic to A, second matrix controls recurrent inputs pre-synaptic to B).\n",
        "Similarly the row-wise decomposition allows us to modulate the post-synaptic recurrent activity of each unit.\n",
        "\n",
        "You could even go as far as have separate neuromodulation for each synapse in the recurrent network.\n",
        "\n",
        "So we have two slightly subtle different parameters here - one is the number of subnetwork units, and for each subnetwork unit is the dimensionality of the neuromodulation onto the hidden network. For example, with two hidden units, $h_t \\in \\mathbb{R}^2$, we could have a single neuromodulator $z_t \\in \\mathbb{R}$ which affects all the recurrent weights the same, $W_x(z(t))\\in \\mathbb{R}$\n",
        "\n",
        "\n",
        "*FOOT:*\n",
        "To keep our notation consistent across architectures, we make the following changes to costacurta et al.'s notation:\n",
        "- $z_t$ is still subnetwork state, but instead of applying relu before weight multiplications, we do so after (this is equivalent in discretised time)\n",
        "- $h_t$ refers our hidden state (the activity which is later decoded for an action) instead of $x_t$ in costacurta et al.\n",
        "- $x_t$ refers to sensory input instead of $u_t$ in costacurta et al.\n",
        "- $W_{xy}$ refers to weight(s) from $x$ to $y$, replacing the A,B,C naming convention in costacurta et al..\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDj3wkUptclW"
      },
      "source": [
        "\n",
        "$$\n",
        "W = U \\Sigma V^\\top = \\sum_{i=1}^{\\mathrm{rank}(W)} \\sigma_i \\, u_i v_i^\\top,\n",
        "\\qquad\n",
        "W = l_1 r_1^\\top + l_2 r_2^\\top \\qquad \\text{(standard SVD)}\n",
        "$$\n",
        "\n",
        "Now we add neuromodulation by $s_t$, which is a number between 0 and 1 for each index $s_i$, so we have\n",
        "\n",
        "$$ W_{rec} = s_1 \\cdot (l_1 r_1^\\top) + s_2\\cdot(l_2 r_2^\\top) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jNSyB2MtclW",
        "outputId": "cfda79ed-71be-45af-b25b-bdd779c3ed28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Row-wise scaled matrix:\n",
            " tensor([[1, 2],\n",
            "        [0, 0]])\n",
            "Column-wise scaled matrix:\n",
            " tensor([[1, 0],\n",
            "        [2, 0]])\n",
            "tensor([[1, 2],\n",
            "        [2, 1]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 2.0000],\n",
              "        [2.0000, 1.0000]])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Different decomopositions and example modulations\n",
        "\n",
        "import torch\n",
        "test_vector1 = torch.tensor([1,0]) #This is one extreme state of s_t for a 2d matrix.\n",
        "\n",
        "test_matrix = torch.tensor([[1,2],[2,1]])\n",
        "# Row-wise (post-synaptic) scaling: multiply each row of the matrix by the corresponding element in the vector\n",
        "row_scaled = test_matrix * test_vector1.unsqueeze(1)\n",
        "print(\"Row-wise scaled matrix:\\n\", row_scaled)\n",
        "# Column-wise (pre-synaptic) scaling: multiply each column of the matrix by the corresponding element in the vector\n",
        "col_scaled = test_matrix * test_vector1\n",
        "print(\"Column-wise scaled matrix:\\n\", col_scaled)\n",
        "\n",
        "U, S, Vh = torch.linalg.svd(test_matrix.float())\n",
        "component_1 = torch.outer(S[0]*U[:, 0], Vh[0, :])\n",
        "component_2 = torch.outer(S[1]*U[:, 1], Vh[1, :])\n",
        "\n",
        "#fig, ax = plt.subplots(1,3)\n",
        "#ax[0].imshow(test_matrix)\n",
        "#ax[1].imshow(component_1)\n",
        "#ax[2].imshow(component_2)\n",
        "print(test_matrix)\n",
        "component_1+component_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "xk47vkMkWps9",
        "outputId": "8e42d31d-85e8-4625-e044-41dc3b5b5dde"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8, 10, 1])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class ManualNMRNN(nn.Module):\n",
        "    def __init__(self, input_size, nm_size, nm_dim, hidden_size, nm_mode='column'):\n",
        "        super().__init__()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "       # assert nm_dim <= nm_size, \"there must be at least as many subnetwork units as nm_dim size\"\n",
        "        self.nm_size = nm_size\n",
        "        self.nm_dim = nm_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.nm_mode = nm_mode  # 'low_rank', 'column', 'row'\n",
        "\n",
        "        # Parameters\n",
        "        self.W_ih = nn.Parameter(torch.Tensor(input_size, hidden_size))      # (I,H)\n",
        "        self.W_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))     # (H,H)\n",
        "        self.W_iz = nn.Parameter(torch.Tensor(input_size, nm_size))          # (I,Zm)\n",
        "        self.W_zz = nn.Parameter(torch.Tensor(nm_size, nm_size))             # (Zm,Zm)\n",
        "        self.W_zk = nn.Parameter(torch.Tensor(nm_size, nm_dim))              # (Zm,K)\n",
        "        self.bias_k = nn.Parameter(torch.Tensor(nm_dim))                     # (K,)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        stdv = 1.0 / math.sqrt(self.hidden_size if self.hidden_size > 0 else 1.0)\n",
        "        for p in self.parameters():\n",
        "            p.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, inputs, init_states=None):\n",
        "        \"\"\"\n",
        "        inputs:  (B, T, I)\n",
        "        returns: hidden_sequence (B, T, H), (h_T, z_T)\n",
        "        \"\"\"\n",
        "        device = inputs.device\n",
        "        B, T, _ = inputs.size()\n",
        "        H = self.hidden_size\n",
        "        Zm = self.nm_size\n",
        "        K = self.nm_dim\n",
        "\n",
        "        # Initial states\n",
        "        if init_states is None:\n",
        "            h_past = torch.zeros(B, H, device=device)\n",
        "            z_past = torch.zeros(B, Zm, device=device)   # <-- important: size nm_size\n",
        "        else:\n",
        "            h_past, z_past = init_states\n",
        "\n",
        "        hidden_sequence = []\n",
        "\n",
        "        # --- Precompute anything that doesn't depend on time ---\n",
        "        # nothing mandatory, except SVD only needed if low_rank is used.\n",
        "        # We'll compute low-rank components on-demand below.\n",
        "\n",
        "        for t in range(T):\n",
        "            x_t = inputs[:, t, :]                                   # (B,I)\n",
        "\n",
        "            # Subnetwork dynamics -> modulation signal s_t\n",
        "            # z_t: (B,Zm), s_t: (B,K)\n",
        "            z_t = self.tanh(z_past @ self.W_zz + x_t @ self.W_iz)   # (B, Zm)\n",
        "            s_t = self.sigmoid(z_t @ self.W_zk + self.bias_k)       # (B, K)\n",
        "\n",
        "            # Build batched recurrent weight W_rec: (B,H,H)\n",
        "            if self.nm_mode == 'low_rank':\n",
        "                W_rec = self._make_Wrec_low_rank(s_t)               # (B,H,H)\n",
        "            elif self.nm_mode == 'column':\n",
        "                W_rec = self._make_Wrec_column(s_t)                 # (B,H,H)\n",
        "            elif self.nm_mode == 'row':\n",
        "                W_rec = self._make_Wrec_row(s_t)                    # (B,H,H)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown nm_mode: {self.nm_mode}\")\n",
        "            # Recurrent step: h_t = tanh( h_{t-1} @ W_rec + x_t @ W_ih )\n",
        "            # Use einsum for batched (B,H) × (B,H,H) -> (B,H)\n",
        "            h_recur = torch.einsum('bi,bij->bj', h_past, W_rec)     # (B,H)\n",
        "            h_t = self.tanh(h_recur + x_t @ self.W_ih )             # (B,H)\n",
        "            hidden_sequence.append(h_t.unsqueeze(1))                # (B,1,H)\n",
        "            # Update states\n",
        "            h_past = h_t\n",
        "            z_past = z_t\n",
        "\n",
        "        hidden_sequence = torch.cat(hidden_sequence, dim=1)         # (B,T,H)\n",
        "        return hidden_sequence, (h_past, z_past)\n",
        "\n",
        "    def _make_Wrec_low_rank(self, s_t):\n",
        "        \"\"\"\n",
        "        Low-rank modulation\n",
        "        s_t: (B, K)  where K = nm_dim\n",
        "        Build W_rec[b] = sum_{k=1..K} s_t[b,k] * sigma_k * u_k v_k^T\n",
        "        \"\"\"\n",
        "        H = self.hidden_size\n",
        "        K = self.nm_dim\n",
        "        assert K <= H, \"nm_dim (number of components) cannot exceed hidden_size\"\n",
        "\n",
        "        # SVD once per forward pass (on CPU/GPU depending on parameter device)\n",
        "        U, S, Vh = torch.linalg.svd(self.W_hh, full_matrices=False)  # U:(H,H), S:(H,), Vh:(H,H)\n",
        "        # take top-K components\n",
        "        U_k = U[:, :K]                 # (H,K)\n",
        "        S_k = S[:K]                    # (K,)\n",
        "        Vh_k = Vh[:K, :]               # (K,H)\n",
        "\n",
        "        # Prebuild the K rank-1 components C[k] = (S_k[k] * U_k[:,k]) ⊗ Vh_k[k,:]\n",
        "        # L = (H,K), R = (K,H) -> C = (K,H,H)\n",
        "        L = U_k * S_k.unsqueeze(0)     # (H,K) broadcast S_k across rows\n",
        "        C = torch.einsum('ik,kj->kij', L, Vh_k)  # (K,H,H)\n",
        "\n",
        "        # Combine per-batch with weights s_t: (B,K) × (K,H,H) -> (B,H,H)\n",
        "        W_rec = torch.einsum('bk,kij->bij', s_t, C)\n",
        "        return W_rec\n",
        "\n",
        "    def _make_Wrec_column(self, s_t):\n",
        "        \"\"\"\n",
        "        Column-wise modulation\n",
        "        - If s_t has shape (B,1): global scalar per batch: W_rec[b] = s_t[b]*W_hh\n",
        "        - If s_t has shape (B,H): per-column scaling: W_rec[b,:,j] = s_t[b,j] * W_hh[:,j]\n",
        "        \"\"\"\n",
        "        B = s_t.shape[0]\n",
        "        H = self.hidden_size\n",
        "        W = self.W_hh.unsqueeze(0).expand(B, H, H)       # (B,H,H)\n",
        "        if s_t.shape[1] == 1:\n",
        "            return W * s_t.view(B, 1, 1)                 # scalar per batch\n",
        "        elif s_t.shape[1] == H:\n",
        "            return W * s_t.view(B, 1, H)                 # broadcast across rows -> scale columns\n",
        "        else:\n",
        "            raise ValueError(f\"column mode expects nm_dim==1 or nm_dim==hidden_size ({H}), got {s_t.shape[1]}\")\n",
        "\n",
        "    def _make_Wrec_row(self, s_t):\n",
        "        \"\"\"\n",
        "        Row-wise modulation\n",
        "        - If s_t has shape (B,1): global scalar per batch: W_rec[b] = s_t[b]*W_hh\n",
        "        - If s_t has shape (B,H): per-row scaling: W_rec[b,i,:] = s_t[b,i] * W_hh[i,:]\n",
        "        \"\"\"\n",
        "        B = s_t.shape[0]\n",
        "        H = self.hidden_size\n",
        "        W = self.W_hh.unsqueeze(0).expand(B, H, H)       # (B,H,H)\n",
        "        if s_t.shape[1] == 1:\n",
        "            return W * s_t.view(B, 1, 1)                 # scalar per batch\n",
        "        elif s_t.shape[1] == H:\n",
        "            return W * s_t.view(B, H, 1)                 # broadcast across cols -> scale rows\n",
        "        else:\n",
        "            raise ValueError(f\"row mode expects nm_dim==1 or nm_dim==hidden_size ({H}), got {s_t.shape[1]}\")\n",
        "\n",
        "\n",
        "test_inputs = torch.ones((8,10,3))\n",
        "testNMRNN = ManualNMRNN(input_size = 3,\n",
        "                       nm_size = 1,\n",
        "                       nm_dim = 1,\n",
        "                       hidden_size = 1,\n",
        "                       nm_mode = 'low_rank')\n",
        "hidden_seq, hidden_tuple = testNMRNN(test_inputs)\n",
        "hidden_seq, hidden_tuple = testNMRNN(test_inputs, init_states = hidden_tuple)\n",
        "#hidden_seq, gate_activations = testLSTM(test_inputs,\n",
        "#                                        return_gate_activations = True)\n",
        "hidden_seq.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCIgC3YoBTW-"
      },
      "source": [
        "### Low-rank decomposition\n",
        "\n",
        "The reason why an NM-RNN scales the low-rank components of the weight matrix, is that it allows it to selectively scale the different dimensions of the recurrent update.\n",
        "\n",
        "Wait a second - what is a low-rank decomposition?\n",
        "Let's take a look at one.\n",
        "\n",
        "We have an example of a very simple 2-unit network with different recurrent dynamics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__Ca3FZ8NcOo",
        "outputId": "49ed8c86-5d5c-48b6-d9a1-733379ed281d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trace = 0.100\n",
            "det   = 0.240\n",
            "Stability: unstable\n",
            "Type: focus\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Potentially useful examples of recurrent weights affecting dynamics\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------- Parameter sets (from your MATLAB) ----------\n",
        "single = {\n",
        "    \"I_y\": np.array([-10, -10]),\n",
        "    \"I_t1\": np.array([0, 20]),\n",
        "    \"I_t2\": np.array([20, 0]),\n",
        "    \"W\": np.array([[-0.2, 0.8],\n",
        "                   [ 0.8,-0.2]]),\n",
        "    \"r_init\": np.array([50, 50]),\n",
        "}\n",
        "\n",
        "multi = {\n",
        "    \"I_y\": np.array([-10, -5]),\n",
        "    \"I_t1\": np.array([0, 20]),\n",
        "    \"I_t2\": np.array([-20, 0]),\n",
        "    \"W\": np.array([[ 1.1,-0.15],\n",
        "                   [-0.15, 1.1]]),\n",
        "    \"r_init\": np.array([50, 50]),\n",
        "}\n",
        "\n",
        "continuous = {\n",
        "    \"I_y\": np.array([-10, -10]),\n",
        "    \"I_t1\": np.array([0.5, 0.0]),\n",
        "    \"I_t2\": np.array([0.5, 0.0]),\n",
        "    \"W\": np.array([[ 0.8,-0.2],\n",
        "                   [-0.2, 0.8]]),\n",
        "    \"r_init\": np.array([50, 50]),\n",
        "}\n",
        "\n",
        "oscillator = {\n",
        "    \"I_y\": np.array([ 8, 20]),\n",
        "    \"I_t1\": np.array([0, 0]),\n",
        "    \"I_t2\": np.array([0,10]),\n",
        "    \"W\": np.array([[ 2.2,-1.3],\n",
        "                   [ 1.2,-0.1]]),\n",
        "    \"r_init\": np.array([50, 50]),\n",
        "}\n",
        "\n",
        "# --------- choose your configuration (match MATLAB: p = continuous) ----------\n",
        "p = oscillator  # or: single, multi, oscillator\n",
        "\n",
        "# ---------- Simulation settings (match MATLAB) ----------\n",
        "dt = 1              # ms\n",
        "t_max = 3000        # ms\n",
        "duration = 250      # ms (length of each stimulation)\n",
        "t1 = 1000           # ms\n",
        "t2 = 2000           # ms\n",
        "dt_tau = 0.1        # Euler ratio for 1 ms step and tau = 10 ms\n",
        "\n",
        "# ---------- Build inputs over time (shape: T+1 by 2) ----------\n",
        "T = int(t_max / dt)\n",
        "times = np.arange(0, T + 1)  # 0..3000 inclusive -> 3001 samples\n",
        "inputs = np.zeros((T + 1, 2))\n",
        "\n",
        "idx1 = int(t1 / dt)\n",
        "idx2 = int(t2 / dt)\n",
        "dur = int(duration / dt)\n",
        "\n",
        "inputs[idx1:idx1 + dur, :] = p[\"I_t1\"]\n",
        "inputs[idx2:idx2 + dur, :] = p[\"I_t2\"]\n",
        "\n",
        "# ---------- Fire-rate simulation (two units, linear-threshold with clipping) ----------\n",
        "N = np.zeros((T + 1, 2))\n",
        "N[0, :] = p[\"r_init\"]\n",
        "\n",
        "W = p[\"W\"]\n",
        "I_y = p[\"I_y\"]\n",
        "\n",
        "for k in range(T):  # update N[k+1] from N[k]\n",
        "    # neuron 1\n",
        "    N[k+1, 0] = N[k, 0] + dt_tau * (-N[k, 0] + W[0, :] @ N[k, :] + inputs[k, 0] - I_y[0])\n",
        "    # neuron 2\n",
        "    N[k+1, 1] = N[k, 1] + dt_tau * (-N[k, 1] + W[1, :] @ N[k, :] + inputs[k, 1] - I_y[1])\n",
        "\n",
        "    # rectify to [0, 100] Hz\n",
        "    N[k+1, :] = np.clip(N[k+1, :], 0.0, 100.0)\n",
        "\n",
        "# ---------- Dynamics classification (same formulas as your MATLAB snippet) ----------\n",
        "trace = -2 + W[0,0] + W[1,1]\n",
        "det = (-1 + W[0,0]) * (-1 + W[1,1]) - W[0,1] * W[1,0]\n",
        "\n",
        "stability = \"unstable\" if trace > 0 else \"stable\"\n",
        "node_or_focus = \"node\" if trace**2 > 4 * det else \"focus\"\n",
        "\n",
        "print(f\"trace = {trace:.3f}\")\n",
        "print(f\"det   = {det:.3f}\")\n",
        "print(f\"Stability: {stability}\")\n",
        "print(f\"Type: {node_or_focus}\")\n",
        "\n",
        "# ---------- Plot (3 panels similar to MATLAB layout) ----------\n",
        "n1_colour = (0.5, 0.7, 1.0, 0.3)\n",
        "n2_colour = (1.0, 0.4, 0.4, 0.3)\n",
        "\n",
        "fig = plt.figure(figsize=(9, 5))\n",
        "gs = fig.add_gridspec(3, 5)\n",
        "\n",
        "# Firing rates over time: subplot(3,5,[6 7]) -> row=1, cols=0:2\n",
        "ax_rates = fig.add_subplot(gs[1, 0:2])\n",
        "ax_rates.plot(times, N[:, 0], color=n1_colour)\n",
        "ax_rates.plot(times, N[:, 1], color=n2_colour)\n",
        "ax_rates.set_xlim(0, T)\n",
        "ax_rates.set_ylabel(\"Firing rate (Hz)\")\n",
        "ax_rates.set_title(\"Firing rates\")\n",
        "\n",
        "# Inputs over time: subplot(3,5,[11 12]) -> row=2, cols=0:2\n",
        "ax_inputs = fig.add_subplot(gs[2, 0:2])\n",
        "ax_inputs.plot(times, inputs[:, 0], color=n1_colour)\n",
        "ax_inputs.plot(times, inputs[:, 1], color=n2_colour)\n",
        "ax_inputs.set_xlim(0, T)\n",
        "ax_inputs.set_ylabel(\"Injected currents\")\n",
        "ax_inputs.set_xlabel(\"Time (ms)\")\n",
        "ax_inputs.set_title(\"Inputs\")\n",
        "\n",
        "# Phase-plane: subplot(3,5,[4 5 9 10 14 15]) -> all rows, cols=3:5 (right block)\n",
        "ax_phase = fig.add_subplot(gs[:, 3:5])\n",
        "c = np.linspace(1, 10, N.shape[0])\n",
        "ax_phase.scatter(N[:, 0], N[:, 1], s=5, c=c)\n",
        "ax_phase.scatter(N[0, 0], N[0, 1], s=50, c='k', marker='+', label='start')\n",
        "ax_phase.scatter(N[-1, 0], N[-1, 1], s=25, c='k', label='end')\n",
        "ax_phase.set_xlim(0, 100)\n",
        "ax_phase.set_ylim(0, 100)\n",
        "ax_phase.set_xlabel(\"Neuron 1 Hz\")\n",
        "ax_phase.set_ylabel(\"Neuron 2 Hz\")\n",
        "ax_phase.set_aspect('equal', adjustable='box')\n",
        "ax_phase.set_title(\"Phase-plane\")\n",
        "ax_phase.legend(loc=\"best\", frameon=False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I3UnSzOg3mz"
      },
      "source": [
        "# Fitting to behaviour\n",
        "\n",
        "We have behavioral data from a two-armed-bandit reversal task.\n",
        "\n",
        "Mice are presented with two cue lights (A and B) and need to make a choice. Probabilities for rewards are 0.75 ('good poke') for one state and 0.25 on the other (not 'good poke'), a state which reverses randomly. However, occasionally only one light goes on (A or B) and the mouse must make their next poke there to continue the task, so this is considered 'forced_choice'.\n",
        "\n",
        "Let's load some data:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhzsatwriXep",
        "outputId": "b34a330d-8b49-4153-ea77-e7150c510d8d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Path' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m DATA_PATH = \u001b[43mPath\u001b[49m(\u001b[33m'\u001b[39m\u001b[33m./NM_TinyRNN/data/\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m TASK = \u001b[33m'\u001b[39m\u001b[33mAB_behaviour\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m subject_data = []\n",
            "\u001b[31mNameError\u001b[39m: name 'Path' is not defined"
          ]
        }
      ],
      "source": [
        "DATA_PATH = Path('./NM_TinyRNN/data/')\n",
        "TASK = 'AB_behaviour'\n",
        "\n",
        "subject_data = []\n",
        "for subject_dir in (DATA_PATH/TASK).iterdir():\n",
        "  if subject_dir.is_dir(): #skip the README file\n",
        "    for session_dir in subject_dir.iterdir():\n",
        "      if 'WS16' in session_dir.stem: #theres duplicate data with weird names/skip this\n",
        "        continue\n",
        "      if session_dir.is_dir(): #skip the .DS_session stuff\n",
        "        subject_data.append(pd.read_csv(session_dir/'trials.htsv', sep = '\\t'))\n",
        "subject_df = pd.concat(subject_data) #concatenate across sessions here (that's ok)\n",
        "\n",
        "subject_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4E3266iitDn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "class AB_Dataset(Dataset):\n",
        "    def __init__(self, subject_data_path, sequence_length):\n",
        "        self.subject_data_path = Path(subject_data_path)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.subject_df = self._load_and_concat_data()\n",
        "        self.inputs, self.targets = self._create_sequences()\n",
        "\n",
        "    def _load_and_concat_data(self):\n",
        "        subject_data = []\n",
        "        for session_dir in self.subject_data_path.iterdir():\n",
        "            if 'WS16' in session_dir.stem:\n",
        "                continue\n",
        "            if session_dir.is_dir():\n",
        "                subject_data.append(pd.read_csv(session_dir/'trials.htsv', sep = '\\t'))\n",
        "        return pd.concat(subject_data)\n",
        "\n",
        "    def _create_sequences(self):\n",
        "        # Convert boolean and categorical columns to numerical\n",
        "        df_processed = self.subject_df.copy()\n",
        "        df_processed['forced_choice'] = df_processed['forced_choice'].astype(int)\n",
        "        df_processed['outcome'] = df_processed['outcome'].astype(int)\n",
        "        df_processed['choice'] = df_processed['choice'].astype('category').cat.codes\n",
        "\n",
        "        # Convert to tensor and handle potential remainder\n",
        "        data_tensor = torch.tensor(df_processed[['forced_choice', 'outcome', 'choice']].values, dtype=torch.float32)\n",
        "        num_rows = data_tensor.size(0)\n",
        "        remainder = num_rows % self.sequence_length\n",
        "        if remainder != 0:\n",
        "            data_tensor = data_tensor[:-remainder] # Trim off the remainder\n",
        "\n",
        "        # Reshape into sequences\n",
        "        num_sequences = data_tensor.size(0) // self.sequence_length\n",
        "        sequences = data_tensor.view(num_sequences, self.sequence_length, data_tensor.size(1))\n",
        "\n",
        "        # Create inputs and targets\n",
        "        # Inputs are 'forced_choice' and 'outcome' at time t\n",
        "        inputs = sequences[:, :-1, :]\n",
        "        # Targets are 'choice' at time t+1, one-hot encoded\n",
        "        targets_codes = sequences[:, 1:, 2].long() # Get the categorical codes as long tensor\n",
        "        targets = torch.nn.functional.one_hot(targets_codes, num_classes=2).float() # One-hot encode\n",
        "\n",
        "\n",
        "        return inputs, targets\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.inputs.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx]\n",
        "\n",
        "# Example usage:\n",
        "# Define the data path and sequence length\n",
        "DATA_PATH = './NM_TinyRNN/data/AB_behaviour/WS16'\n",
        "SEQUENCE_LENGTH = 150+1 # Define your desired sequence length\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "dataset = AB_Dataset(DATA_PATH, SEQUENCE_LENGTH)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Example of iterating through the dataloader\n",
        "# for batch_inputs, batch_targets in dataloader:\n",
        "#      print(\"Inputs shape:\", batch_inputs.shape)\n",
        "#      print(\"Targets shape:\", batch_targets.shape)\n",
        "#      break # Just printing the first batch shapes for demonstration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sc7lAhX4qqNi"
      },
      "outputs": [],
      "source": [
        "# Create a flexible way of training different RNNs to do the task prediction\n",
        "\n",
        "class TinyRNN(nn.Module):\n",
        "  def __init__(self, input_size:int=3, hidden_size:int=1, out_size:int=2,\n",
        "               rnn_type = 'GRU',\n",
        "               sparsity_lambda:float = 1e-2):\n",
        "    super().__init__()\n",
        "    self.I = input_size\n",
        "    self.H = hidden_size\n",
        "    self.O = out_size\n",
        "    self.rnn_type = rnn_type\n",
        "    if rnn_type == 'GRU':\n",
        "      self.rnn = ManualGRU(self.I,self.H)\n",
        "    elif rnn_type == 'LSTM':\n",
        "      self.rnn = ManualLSTM(self.I,self.H)\n",
        "    elif rnn_type == 'NMRNN':\n",
        "      self.rnn = ManualNMRNN(self.I,self.H)\n",
        "    self.decoder = nn.Linear(self.H, self.O)\n",
        "    self.sparsity_lambda = sparsity_lambda\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    hidden, _ = self.rnn(inputs)\n",
        "    predictions = self.decoder(hidden)\n",
        "    return predictions\n",
        "\n",
        "  def compute_losses(self, predictions, targets):\n",
        "    prediction_loss = nn.functional.cross_entropy(predictions, targets) #NB: this applies softmax itself\n",
        "    #for sparsity we need to select the right weights to regularise\n",
        "    sparsity_loss = 0 #init sparsity_loss\n",
        "    for name, param in self.rnn.named_parameters():\n",
        "      if 'bias' not in name:\n",
        "        sparsity_loss += self.sparsity_lambda*torch.abs(param).sum()\n",
        "    return prediction_loss, sparsity_loss\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUxNsKot7GaY"
      },
      "outputs": [],
      "source": [
        "from NM_TinyRNN.code.models import training\n",
        "from importlib import reload; reload(training);\n",
        "DATA_PATH = Path('./NM_TinyRNN/data/rnns')\n",
        "model = TinyRNN(input_size=3, hidden_size=1, out_size=2, rnn_type='GRU', sparsity_lambda = SPARSITY_LAMBDA) # Example sizes\n",
        "trainer = training.Trainer(DATA_PATH/'GRU')\n",
        "trainer.fit(model,dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35lrv5n-7GaY"
      },
      "outputs": [],
      "source": [
        "best_state_dict = torch.load(DATA_PATH/'GRU/1_unit_GRU_L1_0.0001_model_state.pth')\n",
        "model.load_state_dict(best_state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inle6uOO7GaY"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "trial_by_trial_data = {}\n",
        "n_batches, n_seq, n_inputs = dataset.inputs.shape\n",
        "inputs_reshaped = dataset.inputs.reshape(n_batches*n_seq,n_inputs).unsqueeze(0)\n",
        "with torch.no_grad():\n",
        "    predictions = model(inputs_reshaped)\n",
        "    hidden_state, gate_activations = model.rnn(inputs_reshaped, return_gate_activations = True)\n",
        "# add hidden activations\n",
        "for each_unit in range(model.H):\n",
        "    trial_by_trial_data[f'hidden_{each_unit+1}'] = hidden_state[:,:,each_unit].flatten()\n",
        "#add gate activations\n",
        "for gate_label, activations in gate_activations.items():\n",
        "    print(activations.shape)\n",
        "    for each_unit in range(activations.shape[-1]):\n",
        "        trial_by_trial_data[f'gate_{gate_label}_{each_unit+1}'] = activations[:,:,each_unit].flatten()\n",
        "# add logit and probabilities data:\n",
        "log_probs= predictions.log_softmax(dim=2)\n",
        "logits = (log_probs[:,:,0] - log_probs[:,:,1]).flatten()\n",
        "trial_by_trial_data['logit_value'] = logits\n",
        "trial_by_trial_data['logit_change'] = torch.cat([torch.tensor([torch.nan]),(logits[1:]-logits[:-1])])\n",
        "trial_by_trial_data['prob_A'] = torch.exp(log_probs[:,:,0]).flatten()\n",
        "trial_by_trial_data['prob_b'] = torch.exp(log_probs[:,:,1]).flatten()\n",
        "\n",
        "#add whether trial was used in training, validation, or evaluation\n",
        "_,_,_, indices_dict = trainer._split_dataset(dataset)\n",
        "for idx_label, idx_values in indices_dict.items():\n",
        "    # Generate all trial indices for the given batches\n",
        "    trial_indices = np.concatenate([np.arange(idx * n_seq, (idx + 1) * n_seq)\n",
        "                                    for idx in idx_values])\n",
        "    # Create boolean mask using isin\n",
        "    all_trial_indices = np.arange(n_batches * n_seq)\n",
        "    label_indices = np.isin(all_trial_indices, trial_indices)\n",
        "    indices_dict[idx_label] = label_indices\n",
        "trial_by_trial_data.update(indices_dict)\n",
        "df = pd.DataFrame(trial_by_trial_data)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJwyvoa37GaY"
      },
      "outputs": [],
      "source": [
        "hidden_state, gate_activations = model.rnn(inputs_reshaped, return_gate_activations = True)\n",
        "gate_activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGLbKqj57GaY"
      },
      "outputs": [],
      "source": [
        "_, _, _, indices_dict = trainer._split_dataset(dataset)\n",
        "\n",
        "for idx_label, idx_values in indices_dict.items():\n",
        "    # Generate all trial indices for the given batches\n",
        "    trial_indices = np.concatenate([\n",
        "        np.arange(idx * n_seq, (idx + 1) * n_seq)\n",
        "        for idx in idx_values])\n",
        "\n",
        "    # Create boolean mask using isin\n",
        "    all_trial_indices = np.arange(n_batches * n_seq)\n",
        "    label_indices = np.isin(all_trial_indices, trial_indices)\n",
        "\n",
        "    indices_dict[idx_label] = label_indices\n",
        "indices_dict"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}