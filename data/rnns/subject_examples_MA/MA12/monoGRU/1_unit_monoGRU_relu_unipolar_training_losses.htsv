train_prediction	val_pred_losses	train_sparsity	train_energy	train_hebbian	sparsity_lambda	energy_lambda	hebbian_lambda	weight_seed	epoch
0.8290305608197261	0.8081156492233277	4.995346098873573e-07	1.2029040775159223e-11	0.0	1e-05	0.01		1	1
0.7590002856756511	0.7459988594055176	6.104326135563928e-07	0.0	0.0	1e-05	0.01		1	2
0.7182430530849256	0.713249921798706	6.019258762535173e-07	0.0	0.0	1e-05	0.01		1	3
0.7007495008016886	0.6968921184539795	6.167081090480724e-07	0.0	0.0	1e-05	0.01		1	4
0.6939500601668106	0.691884958744049	5.920645333533587e-07	0.0	0.0	1e-05	0.01		1	5
0.6922965112485383	0.6911694765090943	5.774407590682752e-07	0.0	0.0	1e-05	0.01		1	6
0.6920503754364817	0.6900340914726258	5.656251244688879e-07	0.0	0.0	1e-05	0.01		1	7
0.6925645439248335	0.6897997736930848	5.450147359385338e-07	0.0	0.0	1e-05	0.01		1	8
0.6923043853358217	0.6898817539215087	5.186205085967806e-07	0.0	0.0	1e-05	0.01		1	9
0.6924307001264473	0.6901630401611328	5.019909679049306e-07	0.0	0.0	1e-05	0.01		1	10
0.6922897351415533	0.6898197889328003	4.856910502320188e-07	0.0	0.0	1e-05	0.01		1	11
0.692271019283094	0.6899158954620361	4.4509298534553396e-07	0.0	0.0	1e-05	0.01		1	12
0.6928756111546566	0.6897865533828735	4.3893813851450677e-07	0.0	0.0	1e-05	0.01		1	13
0.692821028985475	0.6900054693222046	4.0544612626323026e-07	0.0	0.0	1e-05	0.01		1	14
0.6928891978765787	0.6909040927886964	3.731447143787878e-07	0.0	0.0	1e-05	0.01		1	15
0.6930245317910847	0.6900846004486083	3.515946158777776e-07	0.0	0.0	1e-05	0.01		1	16
0.6926433820473521	0.6905442476272583	3.2005204718045304e-07	0.0	0.0	1e-05	0.01		1	17
0.6925776694950305	0.6904677033424377	2.85284802191056e-07	0.0	0.0	1e-05	0.01		1	18
0.6926194805847972	0.6901499748229981	2.5724020696016897e-07	0.0	0.0	1e-05	0.01		1	19
0.6929126068165428	0.6902117371559142	2.3318280134868762e-07	0.0	0.0	1e-05	0.01		1	20
0.6924550407811217	0.6905748963356018	1.8234145589926096e-07	0.0	0.0	1e-05	0.01		1	21
0.6920190798608881	0.6903469800949096	1.6237209658691807e-07	0.0	0.0	1e-05	0.01		1	22
0.6926043378679376	0.6897979021072387	1.234169463702385e-07	0.0	0.0	1e-05	0.01		1	23
0.6929945192838971	0.6896473884582519	7.740702095678611e-08	0.0	0.0	1e-05	0.01		1	24
0.6923573456312481	0.6902374744415283	5.435532295606257e-08	0.0	0.0	1e-05	0.01		1	25
0.692956927575563	0.6903812170028687	4.8788768300243494e-08	0.0	0.0	1e-05	0.01		1	26
0.6931593512233936	0.6899263501167298	4.1478309629077995e-08	0.0	0.0	1e-05	0.01		1	27
0.692146753009997	0.6907198190689087	4.252841643760443e-08	0.0	0.0	1e-05	0.01		1	28
0.6925360309450249	0.690350341796875	5.087992020685006e-08	0.0	0.0	1e-05	0.01		1	29
0.6925192287093715	0.6901986718177795	3.6656022872032166e-08	0.0	0.0	1e-05	0.01		1	30
0.6925110220909118	0.6900495767593384	4.244531126573747e-08	0.0	0.0	1e-05	0.01		1	31
0.692597586857645	0.6901951670646667	4.563472687454794e-08	0.0	0.0	1e-05	0.01		1	32
0.6928830366385611	0.689997935295105	4.0535138006403996e-08	0.0	0.0	1e-05	0.01		1	33
0.692176138099871	0.6903199195861817	4.2810098016251395e-08	0.0	0.0	1e-05	0.01		1	34
0.6925172617560938	0.690538227558136	4.870377149532751e-08	0.0	0.0	1e-05	0.01		1	35
0.6930259779879921	0.6903560996055602	4.3262153251601515e-08	0.0	0.0	1e-05	0.01		1	36
0.6927235628429211	0.6906728506088256	4.0498003967869815e-08	0.0	0.0	1e-05	0.01		1	37
0.6930939806135078	0.6904729604721069	4.6702888387005114e-08	0.0	0.0	1e-05	0.01		1	38
0.6925401530767743	0.6905538916587829	3.81665955448272e-08	0.0	0.0	1e-05	0.01		1	39
0.6927200493059661	0.6903330087661743	4.2880477554705844e-08	0.0	0.0	1e-05	0.01		1	40
0.6927874966671593	0.6902480363845824	4.601122458474924e-08	0.0	0.0	1e-05	0.01		1	41
0.6926082058956748	0.6910658717155457	4.342049863519031e-08	0.0	0.0	1e-05	0.01		1	42
0.6929720953891154	0.6897711873054504	4.201893139865157e-08	0.0	0.0	1e-05	0.01		1	43
0.6927463192688792	0.6900617241859436	4.768881000036106e-08	0.0	0.0	1e-05	0.01		1	44
