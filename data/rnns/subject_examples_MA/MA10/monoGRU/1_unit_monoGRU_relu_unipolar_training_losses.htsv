train_prediction	val_pred_losses	train_sparsity	train_energy	train_hebbian	sparsity_lambda	energy_lambda	hebbian_lambda	weight_seed	epoch
0.8216407299041747	0.8280704319477081	4.840045064343834e-07	1.3601291253404087e-12	0.0	1e-05	0.01		2	1
0.7582193577990812	0.7636862397193909	6.242087505704737e-07	0.0	0.0	1e-05	0.01		2	2
0.7200919565032509	0.725260004401207	6.22970801219601e-07	0.0	0.0	1e-05	0.01		2	3
0.7022973263964933	0.7063475996255875	6.222958391203171e-07	0.0	0.0	1e-05	0.01		2	4
0.6953864167718327	0.6972640603780746	6.040250435248975e-07	0.0	0.0	1e-05	0.01		2	5
0.694564444177291	0.6924570798873901	5.811639044516008e-07	0.0	0.0	1e-05	0.01		2	6
0.6931339817888598	0.6921259313821793	5.593635438237792e-07	0.0	0.0	1e-05	0.01		2	7
0.6933203725253834	0.6915243566036224	5.380224312081254e-07	0.0	0.0	1e-05	0.01		2	8
0.693238682606641	0.6926518827676773	5.188754043421582e-07	0.0	0.0	1e-05	0.01		2	9
0.6933601077865151	0.6926402300596237	4.926883534875848e-07	0.0	0.0	1e-05	0.01		2	10
0.6933247061336743	0.6909406036138535	4.6882461186428636e-07	0.0	0.0	1e-05	0.01		2	11
0.6932960222749148	0.6919805556535721	4.3851699829514577e-07	0.0	0.0	1e-05	0.01		2	12
0.6932942341355716	0.6917335838079453	4.113842852776168e-07	0.0	0.0	1e-05	0.01		2	13
0.6933959757580476	0.6906912326812744	3.849523228810165e-07	0.0	0.0	1e-05	0.01		2	14
0.6934667720514186	0.6901324242353439	3.510249254144155e-07	0.0	0.0	1e-05	0.01		2	15
0.6931565438999848	0.6914230585098267	3.176876194790084e-07	0.0	0.0	1e-05	0.01		2	16
0.6932159557062036	0.6915040761232376	2.8394744082802127e-07	0.0	0.0	1e-05	0.01		2	17
0.6931770303670098	0.6914557963609695	2.5177580700047284e-07	0.0	0.0	1e-05	0.01		2	18
0.6931614314808565	0.6920184642076492	2.1999431547047766e-07	0.0	0.0	1e-05	0.01		2	19
0.6931926292531629	0.6926772147417068	1.7879321824372762e-07	0.0	0.0	1e-05	0.01		2	20
0.6935714413137997	0.690920278429985	1.4043691135491503e-07	0.0	0.0	1e-05	0.01		2	21
0.6934818555327023	0.6919179260730743	1.0435183593044177e-07	0.0	0.0	1e-05	0.01		2	22
0.6940359508290012	0.6929409354925156	6.454019060363274e-08	0.0	0.0	1e-05	0.01		2	23
0.6938766206012053	0.6907797008752823	5.549899238216686e-08	0.0	0.0	1e-05	0.01		2	24
0.6933667905190413	0.691531166434288	4.782207748753401e-08	0.0	0.0	1e-05	0.01		2	25
0.693336942616631	0.6906268298625946	4.910495058940048e-08	0.0	0.0	1e-05	0.01		2	26
0.6938184114063487	0.6929081231355667	5.091702327738002e-08	0.0	0.0	1e-05	0.01		2	27
0.6931655932875241	0.6916187554597855	4.757315660362885e-08	0.0	0.0	1e-05	0.01		2	28
0.6932835263364455	0.6909689009189606	5.1539231131247265e-08	0.0	0.0	1e-05	0.01		2	29
0.6931927063885857	0.6920506358146667	4.800970277538937e-08	0.0	0.0	1e-05	0.01		2	30
0.6934318858034471	0.6908718943595886	4.7120300027860565e-08	0.0	0.0	1e-05	0.01		2	31
0.6931335224824793	0.6912004351615906	5.079455507186862e-08	0.0	0.0	1e-05	0.01		2	32
0.6936044833239386	0.6909979432821274	4.723293985996273e-08	0.0	0.0	1e-05	0.01		2	33
0.6942801124909344	0.6929491311311722	4.8827054922138394e-08	0.0	0.0	1e-05	0.01		2	34
0.693257030318765	0.6920826435089111	4.623471184990702e-08	0.0	0.0	1e-05	0.01		2	35
