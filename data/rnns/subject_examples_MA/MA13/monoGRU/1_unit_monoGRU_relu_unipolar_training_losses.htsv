train_prediction	val_pred_losses	train_sparsity	train_energy	train_hebbian	sparsity_lambda	energy_lambda	hebbian_lambda	weight_seed	epoch
0.8325304251450759	0.7679641723632813	4.14667041763568e-07	3.954510291875452e-09	0.0	1e-05	0.01		5	1
0.7421345848303573	0.7138444662094117	4.4994214385077107e-07	0.0	0.0	1e-05	0.01		5	2
0.7068200088464297	0.6950641632080078	3.653959514447426e-07	0.0	0.0	1e-05	0.01		5	3
0.693896444944235	0.6916207551956177	2.7449268239584296e-07	0.0	0.0	1e-05	0.01		5	4
0.6910190696899707	0.6916110873222351	1.5705154756172027e-07	0.0	0.0	1e-05	0.01		5	5
0.6909175836122954	0.6916791081428528	5.627724863899176e-08	0.0	0.0	1e-05	0.01		5	6
0.6890175136236045	0.6918924450874329	4.520688896729569e-08	0.0	0.0	1e-05	0.01		5	7
0.6916259252108059	0.6919414281845093	4.628013138530973e-08	0.0	0.0	1e-05	0.01		5	8
0.6880467327741476	0.6917868256568908	4.125176504661436e-08	0.0	0.0	1e-05	0.01		5	9
0.6897368201842673	0.6920906782150269	3.925851459303412e-08	0.0	0.0	1e-05	0.01		5	10
0.6898137193459731	0.6923827528953552	4.5813651060856194e-08	0.0	0.0	1e-05	0.01		5	11
0.687498051386613	0.6922945141792298	4.43723364376489e-08	0.0	0.0	1e-05	0.01		5	12
0.688236543765435	0.6926856875419616	4.219712265077865e-08	0.0	0.0	1e-05	0.01		5	13
0.6931787339540628	0.6920713782310486	3.740461752298731e-08	0.0	0.0	1e-05	0.01		5	14
0.6911053542907423	0.6917227268218994	4.4722985485907876e-08	0.0	0.0	1e-05	0.01		5	15
0.6875678759354812	0.6917915701866151	4.3062002579137456e-08	0.0	0.0	1e-05	0.01		5	16
0.6868103536275716	0.6925411462783814	4.33967189021718e-08	0.0	0.0	1e-05	0.01		5	17
0.6923566987881294	0.6929438948631286	4.1305123082552726e-08	0.0	0.0	1e-05	0.01		5	18
0.6897108027568231	0.6917953372001648	4.478634031156705e-08	0.0	0.0	1e-05	0.01		5	19
0.6889177904679223	0.6922546863555908	4.3417132043434605e-08	0.0	0.0	1e-05	0.01		5	20
0.6895521902121031	0.6920040488243103	4.282047666875472e-08	0.0	0.0	1e-05	0.01		5	21
0.6929060931389148	0.6928904056549072	4.2875401553907565e-08	0.0	0.0	1e-05	0.01		5	22
0.69052765919612	0.6916054129600526	4.4668482724629336e-08	0.0	0.0	1e-05	0.01		5	23
0.6880885981596434	0.6923414707183839	4.2988966437900004e-08	0.0	0.0	1e-05	0.01		5	24
0.6920416699006009	0.692279851436615	4.333940604685548e-08	0.0	0.0	1e-05	0.01		5	25
0.6881894606810348	0.6920437335968018	4.481978514821077e-08	0.0	0.0	1e-05	0.01		5	26
0.6869050516532015	0.692407774925232	4.0389908511756935e-08	0.0	0.0	1e-05	0.01		5	27
0.6867046837623302	0.6929608821868896	4.0881905518994274e-08	0.0	0.0	1e-05	0.01		5	28
0.6913899939793806	0.6936086297035218	4.5599591384888506e-08	0.0	0.0	1e-05	0.01		5	29
0.6898516920896677	0.6922546625137329	4.537473364562261e-08	0.0	0.0	1e-05	0.01		5	30
0.6881290903458228	0.6919837474822998	3.9697042712834204e-08	0.0	0.0	1e-05	0.01		5	31
0.6910926355765417	0.6921009898185729	4.1931550266182364e-08	0.0	0.0	1e-05	0.01		5	32
0.6891648608904618	0.6919949293136597	4.809093841100339e-08	0.0	0.0	1e-05	0.01		5	33
0.6885438744838422	0.6919016718864441	4.46120543242155e-08	0.0	0.0	1e-05	0.01		5	34
0.690684057199038	0.6935628771781921	3.8042534695064816e-08	0.0	0.0	1e-05	0.01		5	35
0.6883941040589259	0.6922685146331787	4.3437405057471345e-08	0.0	0.0	1e-05	0.01		5	36
0.6912580957779516	0.6922160983085632	4.635562156352079e-08	0.0	0.0	1e-05	0.01		5	37
0.6880655403320607	0.6918017745018006	4.2489924023669794e-08	0.0	0.0	1e-05	0.01		5	38
0.6902089371131016	0.6922985315322875	3.9205621860105566e-08	0.0	0.0	1e-05	0.01		5	39
0.6875461179476519	0.6923354983329773	4.398415682953345e-08	0.0	0.0	1e-05	0.01		5	40
0.6867195505362292	0.6927196145057678	4.512705955925446e-08	0.0	0.0	1e-05	0.01		5	41
0.6906441977390877	0.6930878162384033	3.876093415464345e-08	0.0	0.0	1e-05	0.01		5	42
0.690397893006985	0.6924736380577087	4.2956369606682835e-08	0.0	0.0	1e-05	0.01		5	43
