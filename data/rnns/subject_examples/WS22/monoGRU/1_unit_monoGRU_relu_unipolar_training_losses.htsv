train_prediction	val_pred_losses	train_sparsity	train_energy	train_hebbian	sparsity_lambda	energy_lambda	hebbian_lambda	weight_seed	epoch
0.8407759666442871	0.7941993474960327	4.1549847029642706e-07	3.72043819363288e-11	0.0	1e-05	0.01		1	1
0.8204799890518188	0.7780152559280396	4.4464352555451114e-07	0.0	0.0	1e-05	0.01		1	2
0.8026878386735916	0.7633031606674194	5.019435320718912e-07	0.0	0.0	1e-05	0.01		1	3
0.7865299880504608	0.7500398755073547	5.668088505217383e-07	0.0	0.0	1e-05	0.01		1	4
0.7712475210428238	0.7382159233093262	6.023554703915579e-07	0.0	0.0	1e-05	0.01		1	5
0.7574503421783447	0.7279400825500488	6.042996147925805e-07	0.0	0.0	1e-05	0.01		1	6
0.7460319548845291	0.7192126512527466	6.315702165693438e-07	0.0	0.0	1e-05	0.01		1	7
0.7352344244718552	0.7118874788284302	6.183288405736675e-07	0.0	0.0	1e-05	0.01		1	8
0.7259055972099304	0.7059447765350342	6.270622776582968e-07	0.0	0.0	1e-05	0.01		1	9
0.7187106311321259	0.7012132406234741	6.139169528296406e-07	0.0	0.0	1e-05	0.01		1	10
0.7126886546611786	0.697625458240509	6.181299880836377e-07	0.0	0.0	1e-05	0.01		1	11
0.7077327519655228	0.6949546933174133	6.073530158801077e-07	0.0	0.0	1e-05	0.01		1	12
0.7037311494350433	0.6930955648422241	6.152640992240777e-07	0.0	0.0	1e-05	0.01		1	13
0.7006233036518097	0.6918927431106567	6.198416002689555e-07	0.0	0.0	1e-05	0.01		1	14
0.6983955949544907	0.6912003755569458	6.234881482214405e-07	0.0	0.0	1e-05	0.01		1	15
0.6967218071222305	0.6908894181251526	6.084690369334567e-07	0.0	0.0	1e-05	0.01		1	16
0.6954657584428787	0.6908565163612366	6.045865887926993e-07	0.0	0.0	1e-05	0.01		1	17
0.6945916414260864	0.6910009384155273	5.967915512883337e-07	0.0	0.0	1e-05	0.01		1	18
0.6939603090286255	0.6912447810173035	6.135328334266887e-07	0.0	0.0	1e-05	0.01		1	19
0.6936423778533936	0.6915432214736938	6.013528093262721e-07	0.0	0.0	1e-05	0.01		1	20
0.6934276968240738	0.6918321847915649	5.829059546158533e-07	0.0	0.0	1e-05	0.01		1	21
0.693280965089798	0.6921196579933167	5.647868874802953e-07	0.0	0.0	1e-05	0.01		1	22
0.693214476108551	0.6923784613609314	5.675297245488764e-07	0.0	0.0	1e-05	0.01		1	23
0.6931653618812561	0.692583441734314	5.690656621482049e-07	0.0	0.0	1e-05	0.01		1	24
0.6931340843439102	0.6927294135093689	5.792551718286632e-07	0.0	0.0	1e-05	0.01		1	25
0.6931539475917816	0.6928693652153015	5.619235707854386e-07	0.0	0.0	1e-05	0.01		1	26
0.6931364983320236	0.6929300427436829	5.75959717252772e-07	0.0	0.0	1e-05	0.01		1	27
0.6931397914886475	0.6929805278778076	5.415306674194653e-07	0.0	0.0	1e-05	0.01		1	28
0.693140372633934	0.6929927468299866	5.611689033457878e-07	0.0	0.0	1e-05	0.01		1	29
0.6931424140930176	0.6929901838302612	5.565093772474938e-07	0.0	0.0	1e-05	0.01		1	30
0.6931407302618027	0.6930029988288879	5.328263341652928e-07	0.0	0.0	1e-05	0.01		1	31
0.6931410729885101	0.6930113434791565	5.151336068820456e-07	0.0	0.0	1e-05	0.01		1	32
0.6931440383195877	0.6929974555969238	5.351252951868446e-07	0.0	0.0	1e-05	0.01		1	33
0.6931423544883728	0.6930082440376282	5.385600161389448e-07	0.0	0.0	1e-05	0.01		1	34
0.6931454539299011	0.6929537653923035	5.404454697099936e-07	0.0	0.0	1e-05	0.01		1	35
0.6931382715702057	0.6929360628128052	4.954628138875705e-07	0.0	0.0	1e-05	0.01		1	36
0.6931452006101608	0.6928879022598267	5.037101544758116e-07	0.0	0.0	1e-05	0.01		1	37
