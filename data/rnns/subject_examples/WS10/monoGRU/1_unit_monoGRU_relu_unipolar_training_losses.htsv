train_prediction	val_pred_losses	train_sparsity	train_energy	train_hebbian	sparsity_lambda	energy_lambda	hebbian_lambda	weight_seed	epoch
0.8458275198936462	0.8445202708244324	3.98439789250915e-07	4.660269045664301e-12	0.0	1e-05	0.01		2	1
0.8131118535995483	0.819835901260376	4.821356469619786e-07	0.0	0.0	1e-05	0.01		2	2
0.7914290666580199	0.7976096868515015	5.215902035615727e-07	0.0	0.0	1e-05	0.01		2	3
0.7694967627525329	0.7778691649436951	5.911770585953491e-07	0.0	0.0	1e-05	0.01		2	4
0.7525911927223206	0.7608655691146851	6.099467100284528e-07	0.0	0.0	1e-05	0.01		2	5
0.7421932816505432	0.7462677359580994	6.247130613701303e-07	0.0	0.0	1e-05	0.01		2	6
0.7296040534973145	0.734018087387085	6.519330668197654e-07	0.0	0.0	1e-05	0.01		2	7
0.719839608669281	0.7239510416984558	6.428001938729721e-07	0.0	0.0	1e-05	0.01		2	8
0.712775468826294	0.7159085273742676	6.139100264590525e-07	0.0	0.0	1e-05	0.01		2	9
0.7064880013465881	0.709615170955658	6.052169396753015e-07	0.0	0.0	1e-05	0.01		2	10
0.702006459236145	0.7048332095146179	6.421673560907949e-07	0.0	0.0	1e-05	0.01		2	11
0.7000275135040284	0.7013185024261475	6.330693850031821e-07	0.0	0.0	1e-05	0.01		2	12
0.6981055259704589	0.698691189289093	5.953244112788524e-07	0.0	0.0	1e-05	0.01		2	13
0.6952863454818725	0.696726381778717	6.170517849568569e-07	0.0	0.0	1e-05	0.01		2	14
0.6944929838180541	0.6954386830329895	6.207123647072875e-07	0.0	0.0	1e-05	0.01		2	15
0.6943926930427551	0.6945751905441284	5.88106240684283e-07	0.0	0.0	1e-05	0.01		2	16
0.6937765002250672	0.6939213871955872	5.890011266274087e-07	0.0	0.0	1e-05	0.01		2	17
0.6934285402297974	0.6934393048286438	5.999081281515828e-07	0.0	0.0	1e-05	0.01		2	18
0.6931790590286255	0.6931279897689819	5.915384463150985e-07	0.0	0.0	1e-05	0.01		2	19
0.693101406097412	0.6929711103439331	5.739852213082486e-07	0.0	0.0	1e-05	0.01		2	20
0.6932103514671326	0.6928779482841492	5.761382681157556e-07	0.0	0.0	1e-05	0.01		2	21
0.6932617783546448	0.6928511261940002	5.913357313147571e-07	0.0	0.0	1e-05	0.01		2	22
0.6933687210083007	0.6928715705871582	5.779430011898511e-07	0.0	0.0	1e-05	0.01		2	23
0.6932954430580139	0.6929628849029541	5.452090249491448e-07	0.0	0.0	1e-05	0.01		2	24
0.6931638121604919	0.6930753588676453	5.552488801185973e-07	0.0	0.0	1e-05	0.01		2	25
0.6931623339653016	0.6931972503662109	5.571567271545064e-07	0.0	0.0	1e-05	0.01		2	26
0.6931443214416504	0.6932090520858765	5.397593668021728e-07	0.0	0.0	1e-05	0.01		2	27
0.6931902766227723	0.6932585835456848	5.424879191195942e-07	0.0	0.0	1e-05	0.01		2	28
0.6931605815887452	0.6932266354560852	5.501242071659363e-07	0.0	0.0	1e-05	0.01		2	29
0.693159067630768	0.6931902170181274	5.345992917682451e-07	0.0	0.0	1e-05	0.01		2	30
0.6931648969650268	0.6931560635566711	5.007364393350144e-07	0.0	0.0	1e-05	0.01		2	31
0.693134891986847	0.6932147145271301	5.112971280141209e-07	0.0	0.0	1e-05	0.01		2	32
0.6931603074073791	0.693267822265625	5.216066597313329e-07	0.0	0.0	1e-05	0.01		2	33
0.6931566476821899	0.6932692527770996	5.128592420078348e-07	0.0	0.0	1e-05	0.01		2	34
0.6931493878364563	0.6932441592216492	5.00216322052438e-07	0.0	0.0	1e-05	0.01		2	35
0.693180274963379	0.6932597756385803	4.881064455730666e-07	0.0	0.0	1e-05	0.01		2	36
0.6931304693222046	0.6932321190834045	4.879511948274739e-07	0.0	0.0	1e-05	0.01		2	37
0.6931643009185792	0.69321209192276	4.6140272615957654e-07	0.0	0.0	1e-05	0.01		2	38
0.6931674122810363	0.6932311058044434	4.660666263589519e-07	0.0	0.0	1e-05	0.01		2	39
0.69315105676651	0.6932123303413391	4.822566040729726e-07	0.0	0.0	1e-05	0.01		2	40
0.6931299924850464	0.6932121515274048	4.654688098071347e-07	0.0	0.0	1e-05	0.01		2	41
0.6931619405746461	0.6932417750358582	4.407222149893641e-07	0.0	0.0	1e-05	0.01		2	42
