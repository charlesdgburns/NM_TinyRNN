train_prediction	val_pred_losses	train_sparsity	train_energy	train_hebbian	sparsity_lambda	energy_lambda	hebbian_lambda	weight_seed	epoch
0.9027425795793533	0.8667588829994202	3.244735085061734e-07	3.95705591765827e-08	0.0	1e-05	0.01		3	1
0.8783425837755203	0.845170259475708	3.63301598582666e-07	0.0	0.0	1e-05	0.01		3	2
0.856486514210701	0.8250941038131714	4.889519900075356e-07	0.0	0.0	1e-05	0.01		3	3
0.8352695256471634	0.8063724637031555	5.082047636051357e-07	0.0	0.0	1e-05	0.01		3	4
0.8153039067983627	0.789318323135376	4.524480416989718e-07	0.0	0.0	1e-05	0.01		3	5
0.7965109795331955	0.7737266421318054	3.717457701668536e-07	0.0	0.0	1e-05	0.01		3	6
0.7808565348386765	0.7599607706069946	1.8274935342788012e-07	0.0	0.0	1e-05	0.01		3	7
0.7664724290370941	0.7478010654449463	1.0916799730864568e-07	0.0	0.0	1e-05	0.01		3	8
0.7535612285137177	0.7370502948760986	1.389103800875091e-07	0.0	0.0	1e-05	0.01		3	9
0.7419901788234711	0.7278967499732971	1.2274935556888522e-07	0.0	0.0	1e-05	0.01		3	10
0.7320251166820526	0.7200220823287964	1.0253375037905244e-07	0.0	0.0	1e-05	0.01		3	11
0.7236343175172806	0.7134581208229065	1.0860722809979961e-07	0.0	0.0	1e-05	0.01		3	12
0.7165167778730392	0.7080917954444885	1.2929987391885334e-07	0.0	0.0	1e-05	0.01		3	13
0.710516482591629	0.7036967873573303	9.786353061258524e-08	0.0	0.0	1e-05	0.01		3	14
0.7053668051958084	0.7002437710762024	8.385243077668747e-08	0.0	0.0	1e-05	0.01		3	15
0.7015948444604874	0.6974402070045471	8.698032516463172e-08	0.0	0.0	1e-05	0.01		3	16
0.6981075555086136	0.6953665018081665	6.490186432728251e-08	0.0	0.0	1e-05	0.01		3	17
0.6956942975521088	0.6937978267669678	4.724819291368476e-08	0.0	0.0	1e-05	0.01		3	18
0.6933840066194534	0.6927131414413452	8.638112092285155e-08	0.0	0.0	1e-05	0.01		3	19
0.6919605135917664	0.6919353008270264	8.720203936718463e-08	0.0	0.0	1e-05	0.01		3	20
0.6907714903354645	0.6914240121841431	1.0245964610078317e-07	0.0	0.0	1e-05	0.01		3	21
0.689864993095398	0.6911144852638245	1.1709269820414647e-07	0.0	0.0	1e-05	0.01		3	22
0.6890660673379898	0.6909513473510742	9.719530602581017e-08	0.0	0.0	1e-05	0.01		3	23
0.6887097954750061	0.6908871531486511	6.298169985541335e-08	0.0	0.0	1e-05	0.01		3	24
0.6883147954940796	0.6908940076828003	7.993022244079384e-08	0.0	0.0	1e-05	0.01		3	25
0.6884127110242844	0.6909509301185608	6.905416327640523e-08	0.0	0.0	1e-05	0.01		3	26
0.6881148219108582	0.6910320520401001	8.008165863770955e-08	0.0	0.0	1e-05	0.01		3	27
0.6879257559776306	0.6911091208457947	1.1929328636028913e-07	0.0	0.0	1e-05	0.01		3	28
0.6877387017011642	0.6911856532096863	1.0403694084004655e-07	0.0	0.0	1e-05	0.01		3	29
0.6875218600034714	0.6912577748298645	8.885521829427034e-08	0.0	0.0	1e-05	0.01		3	30
0.6876212656497955	0.6913335919380188	9.908434961403145e-08	0.0	0.0	1e-05	0.01		3	31
0.6876102536916733	0.6913992166519165	6.596113788503999e-08	0.0	0.0	1e-05	0.01		3	32
0.6875573694705963	0.6914493441581726	6.933771778960818e-08	0.0	0.0	1e-05	0.01		3	33
0.6877257227897644	0.6914969682693481	9.83114247787853e-08	0.0	0.0	1e-05	0.01		3	34
0.6875734478235245	0.6915445923805237	7.591152861863293e-08	0.0	0.0	1e-05	0.01		3	35
0.6876790672540665	0.6915788650512695	7.96535974956214e-08	0.0	0.0	1e-05	0.01		3	36
0.6873240619897842	0.6915749311447144	1.077935802840102e-07	0.0	0.0	1e-05	0.01		3	37
0.6876543313264847	0.691598117351532	1.0482527201816083e-07	0.0	0.0	1e-05	0.01		3	38
0.6876368373632431	0.6916143894195557	9.301940018247024e-08	0.0	0.0	1e-05	0.01		3	39
0.6876122504472733	0.6916226148605347	9.640470644001198e-08	0.0	0.0	1e-05	0.01		3	40
0.6877748817205429	0.6916414499282837	7.509290522733636e-08	0.0	0.0	1e-05	0.01		3	41
0.6877384930849075	0.6916366815567017	6.967330445917241e-08	0.0	0.0	1e-05	0.01		3	42
0.6875223368406296	0.691626250743866	1.0486710522172871e-07	0.0	0.0	1e-05	0.01		3	43
0.6877938210964203	0.6916440725326538	9.680806556389143e-08	0.0	0.0	1e-05	0.01		3	44
