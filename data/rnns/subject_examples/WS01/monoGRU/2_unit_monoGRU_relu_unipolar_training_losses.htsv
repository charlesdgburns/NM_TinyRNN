train_prediction	val_pred_losses	train_sparsity	train_energy	train_hebbian	sparsity_lambda	energy_lambda	hebbian_lambda	weight_seed	epoch
0.7119991580645244	0.6986352205276489	8.687748523546664e-07	3.1704756879117707e-07	0.0	1e-05	0.01		5	1
0.7060259381930033	0.6959659457206726	1.5620734833040237e-06	1.826811152720135e-06	0.0	1e-05	0.01		5	2
0.7017076810201008	0.6940787434577942	2.567276245220758e-06	5.38878703082446e-06	0.0	1e-05	0.01		5	3
0.6972268223762512	0.6929136514663696	3.6836669702703757e-06	1.4305336662800983e-05	0.0	1e-05	0.01		5	4
0.6937714219093323	0.6922305226325989	4.88419982502819e-06	2.260253192313636e-05	0.0	1e-05	0.01		5	5
0.690582533677419	0.6918735504150391	6.455305083363783e-06	4.442654487017232e-05	0.0	1e-05	0.01		5	6
0.6875808835029602	0.6917194128036499	7.962697054608725e-06	6.443586607929319e-05	0.0	1e-05	0.01		5	7
0.6847295959790548	0.6912819147109985	9.60203124122927e-06	9.153224770367766e-05	0.0	1e-05	0.01		5	8
0.6813732186953226	0.6907185316085815	1.1349169350675462e-05	0.0001456198879168369	0.0	1e-05	0.01		5	9
0.67792675892512	0.6900133490562439	1.3157031389710028e-05	0.0002025126401955883	0.0	1e-05	0.01		5	10
0.6737836996714274	0.688839316368103	1.5001361134636682e-05	0.0002731525358588745	0.0	1e-05	0.01		5	11
0.6692883173624674	0.6875497698783875	1.6955745498610973e-05	0.0003776146719853083	0.0	1e-05	0.01		5	12
0.6641036470731099	0.6863643527030945	1.8951362411219936e-05	0.0005101965022428582	0.0	1e-05	0.01		5	13
0.6589731772740681	0.685526967048645	2.0948489085033845e-05	0.0006406223207401733	0.0	1e-05	0.01		5	14
0.653165598710378	0.684929370880127	2.2808400899521075e-05	0.0007560097922881444	0.0	1e-05	0.01		5	15
0.647847056388855	0.6848034858703613	2.470980204331378e-05	0.0009169347855883341	0.0	1e-05	0.01		5	16
0.6425899664560953	0.6849013566970825	2.6573277864372358e-05	0.0011036170568938057	0.0	1e-05	0.01		5	17
0.637617846330007	0.6849172115325928	2.814330036926549e-05	0.0011926575874288876	0.0	1e-05	0.01		5	18
0.6325042049090067	0.6848903298377991	2.956538022165963e-05	0.0012277604546397924	0.0	1e-05	0.01		5	19
0.6274974346160889	0.6850535273551941	3.0775884321580335e-05	0.0012444907721752922	0.0	1e-05	0.01		5	20
0.6232010126113892	0.6855661869049072	3.192244306167898e-05	0.0012322495070596537	0.0	1e-05	0.01		5	21
0.618735392888387	0.686365008354187	3.31179277660946e-05	0.0012874511691431205	0.0	1e-05	0.01		5	22
0.6146093606948853	0.6875703930854797	3.4413940738886595e-05	0.0014104607592647276	0.0	1e-05	0.01		5	23
0.6112463474273682	0.6887421011924744	3.5623624474586294e-05	0.0014974094616870084	0.0	1e-05	0.01		5	24
0.6081050038337708	0.6897081136703491	3.667580434315217e-05	0.0015327811318760114	0.0	1e-05	0.01		5	25
0.605297287305196	0.6907601356506348	3.7555141413273915e-05	0.0015342623616258306	0.0	1e-05	0.01		5	26
0.6027007500330608	0.6919649243354797	3.8374798653724916e-05	0.0015458985387037196	0.0	1e-05	0.01		5	27
0.6008065342903137	0.6934943795204163	3.909963803986709e-05	0.001532278566931685	0.0	1e-05	0.01		5	28
0.5989042719205221	0.69516521692276	3.986225177262289e-05	0.0015596807158241668	0.0	1e-05	0.01		5	29
0.5971452395121256	0.6966220736503601	4.0523709079328306e-05	0.0015820798774560294	0.0	1e-05	0.01		5	30
0.5957238674163818	0.6979770064353943	4.108741632080637e-05	0.0015837204409763217	0.0	1e-05	0.01		5	31
0.5946779052416484	0.69940584897995	4.1565552237443626e-05	0.0015870357941215238	0.0	1e-05	0.01		5	32
0.5937824447949728	0.7008695006370544	4.2035406901656344e-05	0.0016025919855261843	0.0	1e-05	0.01		5	33
0.5930228034655253	0.7022721767425537	4.244467587947535e-05	0.0016056335686395564	0.0	1e-05	0.01		5	34
0.5923924644788107	0.7034755349159241	4.26993501605466e-05	0.0015755535957093043	0.0	1e-05	0.01		5	35
0.5920256773630778	0.7047471404075623	4.295610900347432e-05	0.0015631680143997073	0.0	1e-05	0.01		5	36
