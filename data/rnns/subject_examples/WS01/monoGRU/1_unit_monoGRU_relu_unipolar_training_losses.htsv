train_prediction	val_pred_losses	train_sparsity	train_energy	train_hebbian	sparsity_lambda	energy_lambda	hebbian_lambda	weight_seed	epoch
0.8833040793736777	0.8283601999282837	4.020375961033551e-07	5.269543597770839e-11	0.0	1e-05	0.01		1	1
0.8659875591595969	0.8142342567443848	4.144206779225594e-07	0.0	0.0	1e-05	0.01		1	2
0.8503108223279319	0.8008158802986145	4.969945356000002e-07	0.0	0.0	1e-05	0.01		1	3
0.8345359365145366	0.7882530689239502	5.011829481797273e-07	0.0	0.0	1e-05	0.01		1	4
0.8199962377548218	0.7764999866485596	5.606478718315581e-07	0.0	0.0	1e-05	0.01		1	5
0.8060617248217263	0.7656174302101135	5.834751088211002e-07	0.0	0.0	1e-05	0.01		1	6
0.793039083480835	0.7556098699569702	6.260705921098028e-07	0.0	0.0	1e-05	0.01		1	7
0.7811480363210042	0.7464580535888672	5.977881680034139e-07	0.0	0.0	1e-05	0.01		1	8
0.770224114259084	0.7381558418273926	6.352192561583554e-07	0.0	0.0	1e-05	0.01		1	9
0.7601025104522705	0.7307102084159851	6.111038904540086e-07	0.0	0.0	1e-05	0.01		1	10
0.7506566047668457	0.72413170337677	6.349325947970404e-07	0.0	0.0	1e-05	0.01		1	11
0.7425973018010458	0.7183090448379517	6.249235298128042e-07	0.0	0.0	1e-05	0.01		1	12
0.7347982128461202	0.7132982611656189	6.144881581349182e-07	0.0	0.0	1e-05	0.01		1	13
0.7284563581148783	0.7089582085609436	6.204037769445373e-07	0.0	0.0	1e-05	0.01		1	14
0.7225296894709269	0.7052940726280212	6.143083434532551e-07	0.0	0.0	1e-05	0.01		1	15
0.7173186937967936	0.7022517919540405	6.124343485680583e-07	0.0	0.0	1e-05	0.01		1	16
0.7125522295633953	0.6997936367988586	6.095912112868973e-07	0.0	0.0	1e-05	0.01		1	17
0.708770215511322	0.6978136897087097	6.26329267561232e-07	0.0	0.0	1e-05	0.01		1	18
0.7056077917416891	0.6962501406669617	6.287820231894632e-07	0.0	0.0	1e-05	0.01		1	19
0.7027866045633951	0.6950684785842896	6.225742671025122e-07	0.0	0.0	1e-05	0.01		1	20
0.7004789511362711	0.6942085027694702	6.195944403467971e-07	0.0	0.0	1e-05	0.01		1	21
0.6984956860542297	0.6936255097389221	6.082865221893977e-07	0.0	0.0	1e-05	0.01		1	22
0.696846882502238	0.6932699680328369	6.011615028000961e-07	0.0	0.0	1e-05	0.01		1	23
0.6955857475598652	0.6930944919586182	6.18621205224675e-07	0.0	0.0	1e-05	0.01		1	24
0.6944759885470072	0.6930634379386902	6.433847564342917e-07	0.0	0.0	1e-05	0.01		1	25
0.6937300364176432	0.6931440234184265	6.079659442548291e-07	0.0	0.0	1e-05	0.01		1	26
0.6929018894831339	0.6932986378669739	6.050530979943385e-07	0.0	0.0	1e-05	0.01		1	27
0.6925178567568461	0.6935170292854309	6.056030391240105e-07	0.0	0.0	1e-05	0.01		1	28
0.6920409202575684	0.6937634944915771	5.77942349385315e-07	0.0	0.0	1e-05	0.01		1	29
0.6917580366134644	0.6940315961837769	5.693838526591813e-07	0.0	0.0	1e-05	0.01		1	30
0.6915097236633301	0.6942998170852661	5.685554545683166e-07	0.0	0.0	1e-05	0.01		1	31
0.6913099884986877	0.6945536136627197	5.626386420469014e-07	0.0	0.0	1e-05	0.01		1	32
0.6912055810292562	0.6948021650314331	5.763609086291884e-07	0.0	0.0	1e-05	0.01		1	33
0.6911051670710245	0.6950287222862244	5.809251698944232e-07	0.0	0.0	1e-05	0.01		1	34
0.6910916566848755	0.6952593922615051	5.838200536345539e-07	0.0	0.0	1e-05	0.01		1	35
0.6909946004549663	0.6954323649406433	5.689004941208017e-07	0.0	0.0	1e-05	0.01		1	36
0.6909711559613546	0.6955897212028503	5.575091487723208e-07	0.0	0.0	1e-05	0.01		1	37
0.6909603873888651	0.6957340836524963	5.73145844858421e-07	0.0	0.0	1e-05	0.01		1	38
0.6909603277842203	0.6958692073822021	5.54517707011352e-07	0.0	0.0	1e-05	0.01		1	39
0.690947691599528	0.6959754228591919	5.613758086534896e-07	0.0	0.0	1e-05	0.01		1	40
0.6909507115681967	0.6960723400115967	5.524265892139132e-07	0.0	0.0	1e-05	0.01		1	41
0.690940797328949	0.6961426138877869	5.447908317061471e-07	0.0	0.0	1e-05	0.01		1	42
0.6909390290578207	0.6961954236030579	5.330940098247083e-07	0.0	0.0	1e-05	0.01		1	43
0.6909510890642802	0.696219265460968	5.512817627580565e-07	0.0	0.0	1e-05	0.01		1	44
0.6909549832344055	0.6962832808494568	5.596036203314725e-07	0.0	0.0	1e-05	0.01		1	45
